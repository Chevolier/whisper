{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f40caf1-4209-48c6-961e-2e097aacae5a",
   "metadata": {},
   "source": [
    "# prepare text and audio_paths files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9345328-38bc-4d65-a911-46d0528dcd98",
   "metadata": {},
   "source": [
    "## preprocess audio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1797e649-f631-4908-bde2-957e7dbe32b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(531, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_name</th>\n",
       "      <th>sentence</th>\n",
       "      <th>user_id</th>\n",
       "      <th>file_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model</td>\n",
       "      <td>CMSN 20 si</td>\n",
       "      <td>wuzy37</td>\n",
       "      <td>8deb96cd-1939-46d0-8f87-5bd798ade923.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model</td>\n",
       "      <td>CMSRO 20di cr</td>\n",
       "      <td>wuzy37</td>\n",
       "      <td>09b311d8-c557-4bfb-bac5-f0d18a5da8e9.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Model</td>\n",
       "      <td>CMSN 20 si</td>\n",
       "      <td>ex_xietian2</td>\n",
       "      <td>6ae92f35-3139-417c-b0f4-d30ac25b3937.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model</td>\n",
       "      <td>CMSRO 20di cr</td>\n",
       "      <td>ex_xietian2</td>\n",
       "      <td>3b65f69e-5aea-4211-acaf-b7bb2f651755.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Model</td>\n",
       "      <td>CMSRO 20di rd</td>\n",
       "      <td>ex_xietian2</td>\n",
       "      <td>c40afd4e-dafb-40a5-8c6e-ce9283a33278.mp3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category_name       sentence      user_id  \\\n",
       "0         Model     CMSN 20 si       wuzy37   \n",
       "1         Model  CMSRO 20di cr       wuzy37   \n",
       "2         Model     CMSN 20 si  ex_xietian2   \n",
       "3         Model  CMSRO 20di cr  ex_xietian2   \n",
       "4         Model  CMSRO 20di rd  ex_xietian2   \n",
       "\n",
       "                                    file_id  \n",
       "0  8deb96cd-1939-46d0-8f87-5bd798ade923.mp3  \n",
       "1  09b311d8-c557-4bfb-bac5-f0d18a5da8e9.mp3  \n",
       "2  6ae92f35-3139-417c-b0f4-d30ac25b3937.mp3  \n",
       "3  3b65f69e-5aea-4211-acaf-b7bb2f651755.mp3  \n",
       "4  c40afd4e-dafb-40a5-8c6e-ce9283a33278.mp3  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "seed = 17\n",
    "random.seed(seed)\n",
    "\n",
    "data_dir = \"data/midea_data_500\"\n",
    "\n",
    "df_trans = pd.read_csv(os.path.join(data_dir, 'transcripts.csv'))\n",
    "\n",
    "print(df_trans.shape)\n",
    "df_trans.rename(columns={'content': 'sentence', 'record_file_id': 'file_id'}, inplace=True)\n",
    "df_trans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abeac1f-2e07-4944-af33-b6ab8fef9d00",
   "metadata": {},
   "source": [
    "## convert amr to mp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d21eea4c-bb0d-4bfc-bf10-308dca490041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "465ca1a8-f847-4d34-95e6-2f6dfb4a1b31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from pydub import AudioSegment\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# for i, row in tqdm(df_trans.iterrows(), total=df_trans.shape[0]):\n",
    "#     filename = row['file_id']\n",
    "#     amr_audio = AudioSegment.from_file(data_dir+f'/amrs/{filename}', format=\"amr\")\n",
    "#     amr_audio.export(data_dir+f'/clips/{filename[:-3]}mp3', format=\"mp3\")\n",
    "#     df_trans.loc[i, 'file_id'] = f'{filename[:-3]}mp3'\n",
    "\n",
    "# df_trans.to_csv(data_dir+'/transcripts.csv', index=False)\n",
    "# df_trans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328714a0-9645-401d-8766-5c9f8386b168",
   "metadata": {},
   "source": [
    "## combine different datasets to form a custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1eafa59-bd3c-46ca-9683-925efd039516",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_dir = 'data/custom_data_v1' # 2k mdcc, 2k cmcc, 300 midea\n",
    "custom_dir = 'data/custom_data_v2' # 65120 mdcc, 8429 cmcc, 300 midea\n",
    "custom_dir = 'data/custom_data_v3' # 65120 mdcc, 8429 cmcc\n",
    "custom_dir = 'data/custom_data_v4' # 5k mdcc, 5k cmcc\n",
    "\n",
    "os.makedirs(custom_dir, exist_ok=True)\n",
    "\n",
    "text_path = os.path.join(custom_dir, 'text')\n",
    "audio_paths_path = os.path.join(custom_dir, 'audio_paths')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba2d104-f618-4344-a9d8-dc7f0084aae2",
   "metadata": {},
   "source": [
    "### MDCC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55640961-2c18-4f9c-a1ce-0b34ecc3fa90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (65120, 4)\n",
      "                                          audio_path  \\\n",
      "0     ./audio/447_1709011939_16988_371.88_376.92.wav   \n",
      "1       ./audio/447_1810221351_22994_56.66_57.71.wav   \n",
      "2     ./audio/447_1711162014_17384_806.04_809.18.wav   \n",
      "3     ./audio/447_1711171151_23819_347.16_349.66.wav   \n",
      "4  ./audio/447_1810221419_57894_113.52_121.42001.wav   \n",
      "\n",
      "                                           text_path     sex  duration  \n",
      "0  ./transcription/447_1709011939_16988_371.88_37...    male   5.04000  \n",
      "1  ./transcription/447_1810221351_22994_56.66_57....    male   1.05000  \n",
      "2  ./transcription/447_1711162014_17384_806.04_80...    male   3.14000  \n",
      "3  ./transcription/447_1711171151_23819_347.16_34...    male   2.50000  \n",
      "4  ./transcription/447_1810221419_57894_113.52_12...  female   7.90001  \n",
      "valid (5663, 4)\n",
      "                                         audio_path  \\\n",
      "0      ./audio/447_1709011939_75569_596.5_600.1.wav   \n",
      "1  ./audio/447_1711171106_19828_2.19996_6.49001.wav   \n",
      "2       ./audio/447_1707171718_21090_18.76_26.3.wav   \n",
      "3    ./audio/447_1711162014_44465_774.63_779.04.wav   \n",
      "4    ./audio/447_1709011939_79382_218.88_221.06.wav   \n",
      "\n",
      "                                           text_path   sex  duration  \n",
      "0  ./transcription/447_1709011939_75569_596.5_600...  male   3.60000  \n",
      "1  ./transcription/447_1711171106_19828_2.19996_6...  male   4.29005  \n",
      "2  ./transcription/447_1707171718_21090_18.76_26....  male   7.54000  \n",
      "3  ./transcription/447_1711162014_44465_774.63_77...  male   4.41000  \n",
      "4  ./transcription/447_1709011939_79382_218.88_22...  male   2.18000  \n",
      "test (12492, 4)\n",
      "                                         audio_path  \\\n",
      "0      ./audio/447_1804041046_13350_13.56_16.64.wav   \n",
      "1    ./audio/447_1804261541_33391_974.16_977.84.wav   \n",
      "2  ./audio/447_2102011200_97296_1912.38_1916.44.wav   \n",
      "3    ./audio/447_1706131232_65761_418.34_421.44.wav   \n",
      "4    ./audio/447_2105311714_21613_106.92_110.16.wav   \n",
      "\n",
      "                                           text_path     sex  duration  \n",
      "0  ./transcription/447_1804041046_13350_13.56_16....  female      3.08  \n",
      "1  ./transcription/447_1804261541_33391_974.16_97...  female      3.68  \n",
      "2  ./transcription/447_2102011200_97296_1912.38_1...    male      4.06  \n",
      "3  ./transcription/447_1706131232_65761_418.34_42...  female      3.10  \n",
      "4  ./transcription/447_2105311714_21613_106.92_11...    male      3.24  \n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import shutil\n",
    "\n",
    "mdcc_dir = 'data/MDCC'\n",
    "\n",
    "splits = ['train', 'valid', 'test']\n",
    "# counts = [2000, 200, 200]\n",
    "counts = [65120, 5663, 12492]\n",
    "counts = [5000, 1000, 1000]\n",
    "\n",
    "for split, count in zip(splits, counts):\n",
    "    save_dir = os.path.join(custom_dir, split)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(mdcc_dir, f'cnt_asr_{split}_metadata.csv'))\n",
    "    print(split, df.shape)\n",
    "    print(df.head())\n",
    "    \n",
    "    df_sample = df.sample(n=min(count, df.shape[0]), random_state=seed)\n",
    "    with open(os.path.join(custom_dir, split, 'text'), 'a') as fo1, open(os.path.join(custom_dir, split, 'audio_paths'), 'a') as fo2:\n",
    "        for i, row in df_sample.iterrows():\n",
    "            unique_id = str(uuid.uuid4())\n",
    "            with open(os.path.join(mdcc_dir, row['text_path'][2:])) as fi:\n",
    "                trans = fi.read()\n",
    "                trans = trans.strip()\n",
    "\n",
    "            audio_path = os.path.realpath(mdcc_dir+'/'+row['audio_path'][2:])\n",
    "\n",
    "            # shutil.copy2(audio_src, audio_dest)\n",
    "            \n",
    "            fo1.write(f\"{unique_id} {trans}\\n\")\n",
    "            fo2.write(f\"{unique_id} {audio_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35db05bb-87cf-468c-96f7-d1eee676d94c",
   "metadata": {},
   "source": [
    "## Common Voice 17.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcf7268f-49b0-4062-ac19-dc058f226f35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (8429, 13)\n",
      "valid (5595, 13)\n",
      "test (5595, 13)\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import shutil\n",
    "\n",
    "common_dir = 'data/cv-corpus-17.0-2024-03-15/zh-HK'\n",
    "splits = ['train', 'valid', 'test']\n",
    "# counts = [2000, 200, 200]\n",
    "# counts = [8429, 5595, 5595]\n",
    "counts = [5000, 1000, 1000]\n",
    "\n",
    "for split, count in zip(splits, counts):\n",
    "    save_dir = os.path.join(custom_dir, split)\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(common_dir, f'{split}.tsv'), sep='\\t')\n",
    "    print(split, df.shape)\n",
    "    # print(df.head())\n",
    "    \n",
    "    df_sample = df.sample(n=min(count, df.shape[0]), random_state=seed)\n",
    "    with open(os.path.join(custom_dir, split, 'text'), 'a') as fo1, open(os.path.join(custom_dir, split, 'audio_paths'), 'a') as fo2:\n",
    "        for i, row in df_sample.iterrows():\n",
    "            unique_id = str(uuid.uuid4())\n",
    "            trans = row['sentence']\n",
    "            audio_path = os.path.realpath(common_dir+'/clips/'+row['path'])\n",
    "\n",
    "            # shutil.copy2(audio_src, audio_dest)\n",
    "\n",
    "            fo1.write(f\"{unique_id} {trans}\\n\")\n",
    "            fo2.write(f\"{unique_id} {audio_path}\\n\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2512f129-1e80-4935-b1c6-0370b145a795",
   "metadata": {},
   "source": [
    "## Midea Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26808f1f-a3c4-473d-a4f8-1a41e30250a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import uuid\n",
    "# import shutil\n",
    "\n",
    "# midea_dir = 'data/midea_data_500'\n",
    "# splits = ['train', 'valid', 'test']\n",
    "# ratios = [0.6, 0.2, 0.2]\n",
    "\n",
    "# df = pd.read_csv(os.path.join(midea_dir, f'transcripts.csv'))\n",
    "# df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "# print(df.shape)\n",
    "# # print(df.head())\n",
    "\n",
    "# prev_count = 0\n",
    "# num = df.shape[0]\n",
    "# for split, ratio in zip(splits, ratios):\n",
    "#     save_dir = os.path.join(custom_dir, split)\n",
    "    \n",
    "#     count = int(num*ratio)\n",
    "#     print(split, count)\n",
    "\n",
    "#     df_sample = df.loc[prev_count:prev_count+count]\n",
    "#     prev_count += count\n",
    "    \n",
    "#     with open(os.path.join(custom_dir, split, 'text'), 'a') as fo1, open(os.path.join(custom_dir, split, 'audio_paths'), 'a') as fo2:\n",
    "#         for i, row in df_sample.iterrows():\n",
    "#             unique_id = str(uuid.uuid4())\n",
    "#             trans = row['sentence']\n",
    "#             audio_path = os.path.realpath(midea_dir+'/clips/'+row['file_id'])\n",
    "\n",
    "#             # shutil.copy2(audio_src, audio_dest)\n",
    "\n",
    "#             fo1.write(f\"{unique_id} {trans}\\n\")\n",
    "#             fo2.write(f\"{unique_id} {audio_path}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06099b9-d91d-48a3-9e63-5e65e93d87ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## remove dirs\n",
    "\n",
    "# shutil.rmtree(custom_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510d87d6-4994-48fe-8bd7-a7e60db9c3ec",
   "metadata": {},
   "source": [
    "# process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3ca7a5d-2f26-4e88-95c1-9cf37cfa7e22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████| 2000/2000 [00:00<00:00, 1047397.68 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 2000/2000 [00:12<00:00, 160.02 examples\n",
      "Data preparation done\n"
     ]
    }
   ],
   "source": [
    "!python3 finetune/custom_data/data_prep.py \\\n",
    "--source_data_dir data/${custom_dir}/test \\\n",
    "--output_data_dir data/${custom_dir}/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4314d15-4099-4d1e-b013-467b132d0c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████| 2000/2000 [00:00<00:00, 1046874.83 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 2000/2000 [00:13<00:00, 153.49 examples\n",
      "Data preparation done\n"
     ]
    }
   ],
   "source": [
    "!python3 finetune/custom_data/data_prep.py \\\n",
    "--source_data_dir data/${custom_dir}/valid \\\n",
    "--output_data_dir data/${custom_dir}/valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8607b02d-ca92-4e7f-92a2-133de4668293",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|████| 10000/10000 [00:00<00:00, 3320379.99 examples/s]\n",
      "Saving the dataset (5/5 shards): 100%|█| 10000/10000 [00:47<00:00, 210.93 exampl\n",
      "Data preparation done\n"
     ]
    }
   ],
   "source": [
    "!python3 finetune/custom_data/data_prep.py \\\n",
    "--source_data_dir data/${custom_dir}/train \\\n",
    "--output_data_dir data/${custom_dir}/train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf62df4-4911-4fcf-a149-00bf8d545fa7",
   "metadata": {},
   "source": [
    "# Try to load the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5114f15d-d8fa-45e4-b273-26d517394eca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1149"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "73549 // (64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3061c9e8-c8b5-4c50-b85d-e3aa9a9dafed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from datasets import DatasetDict, Audio, load_from_disk, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2cab31-3775-4e56-adef-206a1bd85c1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_datasets = ['data/custom_data_v1/train']\n",
    "eval_datasets = ['data/custom_data_v1/valid']\n",
    "\n",
    "def load_custom_dataset(split):\n",
    "    ds = []\n",
    "    if split == 'train':\n",
    "        for dset in train_datasets:\n",
    "            ds.append(load_from_disk(dset))\n",
    "    if split == 'eval':\n",
    "        for dset in eval_datasets:\n",
    "            ds.append(load_from_disk(dset))\n",
    "\n",
    "    ds_to_return = concatenate_datasets(ds)\n",
    "    ds_to_return = ds_to_return.shuffle(seed=22)\n",
    "    return ds_to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca44366f-20f2-40b7-84d5-fa867feb671f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_to_return = load_custom_dataset('eval')\n",
    "ds_to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251472cf-dc30-49f3-81a1-2b820b67896e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for ex in ds_to_return:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f1afbd-e24d-4b22-9559-0ab7418aad99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and (possibly) resample audio data to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array \n",
    "    batch[\"input_features\"] = processor.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    # compute input length of audio sample in seconds\n",
    "    batch[\"input_length\"] = len(audio[\"array\"]) / audio[\"sampling_rate\"]\n",
    "    \n",
    "    # optional pre-processing steps\n",
    "    transcription = batch[\"sentence\"]\n",
    "    if do_lower_case:\n",
    "        transcription = transcription.lower()\n",
    "    if do_remove_punctuation:\n",
    "        transcription = normalizer(transcription).strip()\n",
    "    \n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = processor.tokenizer(transcription).input_ids\n",
    "    return batch\n",
    "\n",
    "max_label_length = 225 # model.config.max_length\n",
    "min_input_length = 0.0\n",
    "max_input_length = 30.0\n",
    "def is_in_length_range(length, labels):\n",
    "    return min_input_length < length < max_input_length and 0 < len(labels) < max_label_length\n",
    "\n",
    "\n",
    "print('DATASET PREPARATION IN PROGRESS...')\n",
    "raw_dataset = DatasetDict()\n",
    "# raw_dataset[\"train\"] = load_custom_dataset('train')\n",
    "raw_dataset[\"eval\"] = load_custom_dataset('eval')\n",
    "\n",
    "raw_dataset = raw_dataset.cast_column(\"audio\", Audio(sampling_rate=args.sampling_rate))\n",
    "raw_dataset = raw_dataset.map(prepare_dataset, num_proc=args.num_proc)\n",
    "\n",
    "raw_dataset = raw_dataset.filter(\n",
    "    is_in_length_range,\n",
    "    input_columns=[\"input_length\", \"labels\"],\n",
    "    num_proc=args.num_proc,\n",
    ")\n",
    "\n",
    "###############################     DATA COLLATOR AND METRIC DEFINITION     ########################\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "print('DATASET PREPARATION COMPLETED')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffe5026-0f87-4d10-952e-f1e44dea6c82",
   "metadata": {},
   "source": [
    "# finetuning on huggingface dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c4d188-216c-460d-afc8-1ee0443e2044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d43b3c-291d-4cbb-b6f8-926fbb9c06c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !huggingface-cli login \n",
    "\n",
    "# get your account token from https://huggingface.co/settings/tokens\n",
    "token = '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'\n",
    "\n",
    "# # import the relavant libraries for loggin in\n",
    "# from huggingface_hub import HfApi, HfFolder\n",
    "\n",
    "# # set api for login and save token\n",
    "# api=HfApi()\n",
    "# api.set_access_token(token)\n",
    "# folder = HfFolder()\n",
    "# folder.save_token(token)\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ed6160-5a9b-4e1a-b48d-bd499b5abc90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile train_hf.sh\n",
    "\n",
    "ngpu=1  # number of GPUs to perform distributed training on.\n",
    "\n",
    "torchrun --nproc_per_node=${ngpu} finetune/train/fine-tune_on_hf_dataset.py \\\n",
    "--model_name /home/ec2-user/SageMaker/efs/Models/whisper-large-v3 \\\n",
    "--language Cantonese \\\n",
    "--sampling_rate 16000 \\\n",
    "--num_proc ${ngpu} \\\n",
    "--train_strategy steps \\\n",
    "--learning_rate 3e-3 \\\n",
    "--warmup 1000 \\\n",
    "--train_batchsize 1 \\\n",
    "--eval_batchsize 1 \\\n",
    "--num_steps 10000 \\\n",
    "--resume_from_ckpt None \\\n",
    "--output_dir checkpoint \\\n",
    "--train_datasets mozilla-foundation/common_voice_17_0  \\\n",
    "--train_dataset_configs yue \\\n",
    "--train_dataset_splits validation \\\n",
    "--train_dataset_text_columns sentence \\\n",
    "--eval_datasets mozilla-foundation/common_voice_17_0 \\\n",
    "--eval_dataset_configs yue \\\n",
    "--eval_dataset_splits test \\\n",
    "--eval_dataset_text_columns sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901e3519-b6a9-430c-bcc3-6dec2a25f919",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!bash train_hf.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c9810a-1f59-4282-805b-5dec6181fcd1",
   "metadata": {},
   "source": [
    "# finetuning on custom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490a454a-47d3-4ec1-971a-6c01a5708855",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile train.sh\n",
    "\n",
    "ngpu=1  # number of GPUs to perform distributed training on.\n",
    "\n",
    "torchrun --nproc_per_node=${ngpu} finetune/train/fine-tune_on_custom_dataset.py \\\n",
    "--model_name /home/ec2-user/SageMaker/efs/Models/whisper-large-v3 \\\n",
    "--language Cantonese \\\n",
    "--sampling_rate 16000 \\\n",
    "--num_proc ${ngpu} \\\n",
    "--train_strategy epoch \\\n",
    "--learning_rate 3e-3 \\\n",
    "--warmup 1000 \\\n",
    "--train_batchsize 1 \\\n",
    "--eval_batchsize 1 \\\n",
    "--num_epochs 2 \\\n",
    "--resume_from_ckpt None \\\n",
    "--output_dir checkpoint \\\n",
    "--train_datasets data/midea_data \\\n",
    "--eval_datasets data/midea_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a73c79c-c8da-4f95-b5b0-46d96b698f6f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!bash train.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85085b99-e2b2-42b2-a12d-afb63470c188",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_whisper_py310",
   "language": "python",
   "name": "conda_whisper_py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
