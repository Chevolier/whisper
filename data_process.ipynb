{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f40caf1-4209-48c6-961e-2e097aacae5a",
   "metadata": {},
   "source": [
    "# prepare text and audio_paths files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9345328-38bc-4d65-a911-46d0528dcd98",
   "metadata": {},
   "source": [
    "## preprocess audio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1797e649-f631-4908-bde2-957e7dbe32b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2173, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_name</th>\n",
       "      <th>sentence</th>\n",
       "      <th>user_id</th>\n",
       "      <th>file_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model</td>\n",
       "      <td>CMSN 20 si</td>\n",
       "      <td>yeungchimkuen</td>\n",
       "      <td>2956c101-d82f-4f2c-8f1b-6119fa87694d.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model</td>\n",
       "      <td>CMSRO 20di cr</td>\n",
       "      <td>yeungchimkuen</td>\n",
       "      <td>c124f9b4-38e1-4cee-8b41-49f57f069806.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Model</td>\n",
       "      <td>CMSRO 20di rd</td>\n",
       "      <td>yeungchimkuen</td>\n",
       "      <td>17f3f07c-eca6-41d8-a3e4-9c0120595eb3.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model</td>\n",
       "      <td>AF-52CS1TRHK(H)</td>\n",
       "      <td>yeungchimkuen</td>\n",
       "      <td>5102309c-8fb3-4232-b257-2bd8ed6ef53e.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Model</td>\n",
       "      <td>AF-74CS1TRHK(H)</td>\n",
       "      <td>yeungchimkuen</td>\n",
       "      <td>51ab811e-12a0-47b6-834d-939b8b4ec144.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category_name         sentence        user_id  \\\n",
       "0         Model       CMSN 20 si  yeungchimkuen   \n",
       "1         Model    CMSRO 20di cr  yeungchimkuen   \n",
       "2         Model    CMSRO 20di rd  yeungchimkuen   \n",
       "3         Model  AF-52CS1TRHK(H)  yeungchimkuen   \n",
       "4         Model  AF-74CS1TRHK(H)  yeungchimkuen   \n",
       "\n",
       "                                    file_id  \n",
       "0  2956c101-d82f-4f2c-8f1b-6119fa87694d.wav  \n",
       "1  c124f9b4-38e1-4cee-8b41-49f57f069806.wav  \n",
       "2  17f3f07c-eca6-41d8-a3e4-9c0120595eb3.wav  \n",
       "3  5102309c-8fb3-4232-b257-2bd8ed6ef53e.wav  \n",
       "4  51ab811e-12a0-47b6-834d-939b8b4ec144.wav  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "seed = 17\n",
    "random.seed(seed)\n",
    "\n",
    "data_dir = \"data/midea_2173\"   # your midea data directory\n",
    "\n",
    "df_trans = pd.read_csv(os.path.join(data_dir, 'transcripts.csv'))\n",
    "\n",
    "print(df_trans.shape)\n",
    "df_trans.rename(columns={'content': 'sentence', 'record_file_id': 'file_id'}, inplace=True)\n",
    "df_trans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abeac1f-2e07-4944-af33-b6ab8fef9d00",
   "metadata": {},
   "source": [
    "## convert amr to wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d21eea4c-bb0d-4bfc-bf10-308dca490041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "465ca1a8-f847-4d34-95e6-2f6dfb4a1b31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i, row in tqdm(df_trans.iterrows(), total=df_trans.shape[0]):\n",
    "    filename = row['file_id'][:-3]\n",
    "    amr_audio = AudioSegment.from_file(data_dir+f'/amrs/{filename}amr', format=\"amr\")\n",
    "    amr_audio.export(data_dir+f'/wavs/{filename}wav', format=\"wav\")\n",
    "    df_trans.loc[i, 'file_id'] = f'{filename}wav'\n",
    "\n",
    "df_trans.to_csv(data_dir+'/transcripts.csv', index=False)\n",
    "df_trans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328714a0-9645-401d-8766-5c9f8386b168",
   "metadata": {},
   "source": [
    "## combine different datasets to form a custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a1eafa59-bd3c-46ca-9683-925efd039516",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# custom_dir = 'data/custom_data_v1' # 2k mdcc, 2k cmcc, 318 midea\n",
    "# custom_dir = 'data/custom_data_v2' # 65120 mdcc, 8429 cmcc, 300 midea\n",
    "# custom_dir = 'data/custom_data_v3' # 65120 mdcc, 8429 cmcc\n",
    "# custom_dir = 'data/custom_data_v4' # 5k mdcc, 5k cmcc\n",
    "# custom_dir = 'data/custom_data_v5' # 2k mdcc, 2k cmcc\n",
    "# custom_dir = 'data/custom_data_v6' # 5k mdcc, 5k cmcc, 318 midea\n",
    "custom_dir = 'data/custom_data_v7' # 5k mdcc, 5k cmcc, 1303 midea\n",
    "# custom_dir = 'data/custom_data_v8' # 1k mdcc, 1k cmcc, 1303 midea\n",
    "\n",
    "os.makedirs(custom_dir, exist_ok=True)\n",
    "\n",
    "text_path = os.path.join(custom_dir, 'text')\n",
    "audio_paths_path = os.path.join(custom_dir, 'audio_paths')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba2d104-f618-4344-a9d8-dc7f0084aae2",
   "metadata": {},
   "source": [
    "### MDCC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "55640961-2c18-4f9c-a1ce-0b34ecc3fa90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (65120, 4)\n",
      "                                          audio_path  \\\n",
      "0     ./audio/447_1709011939_16988_371.88_376.92.wav   \n",
      "1       ./audio/447_1810221351_22994_56.66_57.71.wav   \n",
      "2     ./audio/447_1711162014_17384_806.04_809.18.wav   \n",
      "3     ./audio/447_1711171151_23819_347.16_349.66.wav   \n",
      "4  ./audio/447_1810221419_57894_113.52_121.42001.wav   \n",
      "\n",
      "                                           text_path     sex  duration  \n",
      "0  ./transcription/447_1709011939_16988_371.88_37...    male   5.04000  \n",
      "1  ./transcription/447_1810221351_22994_56.66_57....    male   1.05000  \n",
      "2  ./transcription/447_1711162014_17384_806.04_80...    male   3.14000  \n",
      "3  ./transcription/447_1711171151_23819_347.16_34...    male   2.50000  \n",
      "4  ./transcription/447_1810221419_57894_113.52_12...  female   7.90001  \n",
      "valid (5663, 4)\n",
      "                                         audio_path  \\\n",
      "0      ./audio/447_1709011939_75569_596.5_600.1.wav   \n",
      "1  ./audio/447_1711171106_19828_2.19996_6.49001.wav   \n",
      "2       ./audio/447_1707171718_21090_18.76_26.3.wav   \n",
      "3    ./audio/447_1711162014_44465_774.63_779.04.wav   \n",
      "4    ./audio/447_1709011939_79382_218.88_221.06.wav   \n",
      "\n",
      "                                           text_path   sex  duration  \n",
      "0  ./transcription/447_1709011939_75569_596.5_600...  male   3.60000  \n",
      "1  ./transcription/447_1711171106_19828_2.19996_6...  male   4.29005  \n",
      "2  ./transcription/447_1707171718_21090_18.76_26....  male   7.54000  \n",
      "3  ./transcription/447_1711162014_44465_774.63_77...  male   4.41000  \n",
      "4  ./transcription/447_1709011939_79382_218.88_22...  male   2.18000  \n",
      "test (12492, 4)\n",
      "                                         audio_path  \\\n",
      "0      ./audio/447_1804041046_13350_13.56_16.64.wav   \n",
      "1    ./audio/447_1804261541_33391_974.16_977.84.wav   \n",
      "2  ./audio/447_2102011200_97296_1912.38_1916.44.wav   \n",
      "3    ./audio/447_1706131232_65761_418.34_421.44.wav   \n",
      "4    ./audio/447_2105311714_21613_106.92_110.16.wav   \n",
      "\n",
      "                                           text_path     sex  duration  \n",
      "0  ./transcription/447_1804041046_13350_13.56_16....  female      3.08  \n",
      "1  ./transcription/447_1804261541_33391_974.16_97...  female      3.68  \n",
      "2  ./transcription/447_2102011200_97296_1912.38_1...    male      4.06  \n",
      "3  ./transcription/447_1706131232_65761_418.34_42...  female      3.10  \n",
      "4  ./transcription/447_2105311714_21613_106.92_11...    male      3.24  \n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import shutil\n",
    "\n",
    "mdcc_dir = 'data/MDCC'\n",
    "\n",
    "splits = ['train', 'valid', 'test']\n",
    "# counts = [2000, 200, 200]\n",
    "# counts = [65120, 5663, 12492]\n",
    "# counts = [65120, 5663, 12492]\n",
    "# counts = [5000, 1000, 1000]\n",
    "# counts = [2000, 200, 200]\n",
    "# counts = [5000, 200, 200]\n",
    "counts = [5000, 500, 500]\n",
    "# counts = [1000, 200, 200]\n",
    "\n",
    "for split, count in zip(splits, counts):\n",
    "    save_dir = os.path.join(custom_dir, split)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(mdcc_dir, f'cnt_asr_{split}_metadata.csv'))\n",
    "    print(split, df.shape)\n",
    "    print(df.head())\n",
    "    \n",
    "    df_sample = df.sample(n=min(count, df.shape[0]), random_state=seed)\n",
    "    with open(os.path.join(custom_dir, split, 'text'), 'a') as fo1, open(os.path.join(custom_dir, split, 'audio_paths'), 'a') as fo2:\n",
    "        for i, row in df_sample.iterrows():\n",
    "            unique_id = str(uuid.uuid4())\n",
    "            with open(os.path.join(mdcc_dir, row['text_path'][2:])) as fi:\n",
    "                trans = fi.read()\n",
    "                trans = trans.strip()\n",
    "\n",
    "            audio_path = os.path.realpath(mdcc_dir+'/'+row['audio_path'][2:])\n",
    "\n",
    "            # shutil.copy2(audio_src, audio_dest)\n",
    "            \n",
    "            # if i < df_sample.shape[0] - 1:\n",
    "            fo1.write(f\"{unique_id} {trans}\\n\")\n",
    "            fo2.write(f\"{unique_id} {audio_path}\\n\")\n",
    "            # else:\n",
    "            #     fo1.write(f\"{unique_id} {trans}\\n\")\n",
    "            #     fo2.write(f\"{unique_id} {audio_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35db05bb-87cf-468c-96f7-d1eee676d94c",
   "metadata": {},
   "source": [
    "## Common Voice 17.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fcf7268f-49b0-4062-ac19-dc058f226f35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (8429, 13)\n",
      "valid (5595, 13)\n",
      "test (5595, 13)\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import shutil\n",
    "\n",
    "common_dir = 'data/cv-corpus-17.0-2024-03-15/zh-HK'\n",
    "splits = ['train', 'valid', 'test']\n",
    "# counts = [2000, 200, 200]\n",
    "# counts = [8429, 5595, 5595]\n",
    "# counts = [8429, 5595, 5595]\n",
    "# counts = [5000, 1000, 1000]\n",
    "# counts = [2000, 200, 200]\n",
    "# counts = [5000, 200, 200]\n",
    "counts = [5000, 500, 500]\n",
    "# counts = [1000, 200, 200]\n",
    "\n",
    "for split, count in zip(splits, counts):\n",
    "    save_dir = os.path.join(custom_dir, split)\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(common_dir, f'{split}.tsv'), sep='\\t')\n",
    "    print(split, df.shape)\n",
    "    # print(df.head())\n",
    "    \n",
    "    df_sample = df.sample(n=min(count, df.shape[0]), random_state=seed)\n",
    "    with open(os.path.join(custom_dir, split, 'text'), 'a') as fo1, open(os.path.join(custom_dir, split, 'audio_paths'), 'a') as fo2:\n",
    "        for i, row in df_sample.iterrows():\n",
    "            unique_id = str(uuid.uuid4())\n",
    "            trans = row['sentence']\n",
    "            audio_path = os.path.realpath(common_dir+'/clips/'+row['path'])\n",
    "\n",
    "            # shutil.copy2(audio_src, audio_dest)\n",
    "\n",
    "            # if i < df_sample.shape[0] - 1:\n",
    "            fo1.write(f\"{unique_id} {trans}\\n\")\n",
    "            fo2.write(f\"{unique_id} {audio_path}\\n\")\n",
    "            # else:\n",
    "            #     fo1.write(f\"{unique_id} {trans}\\n\")\n",
    "            #     fo2.write(f\"{unique_id} {audio_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2512f129-1e80-4935-b1c6-0370b145a795",
   "metadata": {},
   "source": [
    "## Midea Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26808f1f-a3c4-473d-a4f8-1a41e30250a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2173, 4)\n",
      "      category_name      sentence           user_id  \\\n",
      "0             Model  AW-7480H(IL)  ex_hangyee.kwong   \n",
      "1           Surname             盛  ex_hangyee.kwong   \n",
      "2             Model   RC-5SLIH(W)     yeungchimkuen   \n",
      "3  Product Category           氣炸鍋     yeungchimkuen   \n",
      "4             Model        TAS-X3     yeungchimkuen   \n",
      "\n",
      "                                    file_id  \n",
      "0  16193c60-6ddf-4647-a9f1-4c576ec73076.wav  \n",
      "1  5aae676f-f5f6-4d85-8a54-33f3d98b4b83.wav  \n",
      "2  d935c585-8b32-4abf-9306-081f0ca41256.wav  \n",
      "3  e5c00f0c-81f4-43a1-8c00-987ee1952c89.wav  \n",
      "4  ce627395-3fa6-4ec8-94dd-b42b1fc700b9.wav  \n",
      "train 1303\n",
      "valid 434\n",
      "test 434\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import shutil\n",
    "\n",
    "midea_dir = 'data/midea_2173'\n",
    "splits = ['train', 'valid', 'test']\n",
    "ratios = [0.6, 0.2, 0.2]\n",
    "\n",
    "df = pd.read_csv(os.path.join(midea_dir, f'transcripts.csv'))\n",
    "df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "print(df.shape)\n",
    "print(df.head())\n",
    "\n",
    "prev_count = 0\n",
    "num = df.shape[0]\n",
    "for split, ratio in zip(splits, ratios):\n",
    "    save_dir = os.path.join(custom_dir, split)\n",
    "    \n",
    "    count = int(num*ratio)\n",
    "    print(split, count)\n",
    "\n",
    "    df_sample = df.loc[prev_count:prev_count+count]\n",
    "    prev_count += count\n",
    "    \n",
    "    with open(os.path.join(custom_dir, split, 'text'), 'a') as fo1, open(os.path.join(custom_dir, split, 'audio_paths'), 'a') as fo2:\n",
    "        for i, row in df_sample.iterrows():\n",
    "            unique_id = str(uuid.uuid4())\n",
    "            trans = row['sentence']\n",
    "            audio_path = os.path.realpath(midea_dir+'/wavs/'+row['file_id'])\n",
    "\n",
    "            # shutil.copy2(audio_src, audio_dest)\n",
    "\n",
    "            # if i < df_sample.shape[0] - 1:\n",
    "            fo1.write(f\"{unique_id} {trans}\\n\")\n",
    "            fo2.write(f\"{unique_id} {audio_path}\\n\")\n",
    "            # else:\n",
    "            #     fo1.write(f\"{unique_id} {trans}\")\n",
    "            #     fo2.write(f\"{unique_id} {audio_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a06099b9-d91d-48a3-9e63-5e65e93d87ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # remove dirs\n",
    "\n",
    "# shutil.rmtree(custom_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510d87d6-4994-48fe-8bd7-a7e60db9c3ec",
   "metadata": {},
   "source": [
    "# process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b3ca7a5d-2f26-4e88-95c1-9cf37cfa7e22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 835/835 [00:00<00:00, 24586.98 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 835/835 [00:03<00:00, 242.64 examples/s\n",
      "Data preparation done\n"
     ]
    }
   ],
   "source": [
    "!python3 finetune/custom_data/data_prep.py \\\n",
    "--source_data_dir data/${custom_dir}/test \\\n",
    "--output_data_dir data/${custom_dir}/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c4314d15-4099-4d1e-b013-467b132d0c29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|█████████| 835/835 [00:00<00:00, 512098.82 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 835/835 [00:05<00:00, 159.94 examples/s\n",
      "Data preparation done\n"
     ]
    }
   ],
   "source": [
    "!python3 finetune/custom_data/data_prep.py \\\n",
    "--source_data_dir data/${custom_dir}/valid \\\n",
    "--output_data_dir data/${custom_dir}/valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8607b02d-ca92-4e7f-92a2-133de4668293",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████| 3304/3304 [00:00<00:00, 1630734.34 examples/s]\n",
      "Saving the dataset (2/2 shards): 100%|█| 3304/3304 [00:20<00:00, 160.18 examples\n",
      "Data preparation done\n"
     ]
    }
   ],
   "source": [
    "!python3 finetune/custom_data/data_prep.py \\\n",
    "--source_data_dir data/${custom_dir}/train \\\n",
    "--output_data_dir data/${custom_dir}/train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf62df4-4911-4fcf-a149-00bf8d545fa7",
   "metadata": {},
   "source": [
    "# Try to load the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a820b7-58e0-40b7-b33e-9ab3ccd38653",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import argparse\n",
    "# from datasets import DatasetDict, Audio, load_from_disk, concatenate_datasets\n",
    "\n",
    "# train_datasets = ['data/custom_data_v1/train']\n",
    "# eval_datasets = ['data/custom_data_v1/valid']\n",
    "\n",
    "# def load_custom_dataset(split):\n",
    "#     ds = []\n",
    "#     if split == 'train':\n",
    "#         for dset in train_datasets:\n",
    "#             ds.append(load_from_disk(dset))\n",
    "#     if split == 'eval':\n",
    "#         for dset in eval_datasets:\n",
    "#             ds.append(load_from_disk(dset))\n",
    "\n",
    "#     ds_to_return = concatenate_datasets(ds)\n",
    "#     ds_to_return = ds_to_return.shuffle(seed=22)\n",
    "#     return ds_to_return\n",
    "\n",
    "# ds_to_return = load_custom_dataset('eval')\n",
    "# ds_to_return\n",
    "\n",
    "# for ex in ds_to_return:\n",
    "#     print(ex)\n",
    "\n",
    "# def prepare_dataset(batch):\n",
    "#     # load and (possibly) resample audio data to 16kHz\n",
    "#     audio = batch[\"audio\"]\n",
    "\n",
    "#     # compute log-Mel input features from input audio array \n",
    "#     batch[\"input_features\"] = processor.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "#     # compute input length of audio sample in seconds\n",
    "#     batch[\"input_length\"] = len(audio[\"array\"]) / audio[\"sampling_rate\"]\n",
    "    \n",
    "#     # optional pre-processing steps\n",
    "#     transcription = batch[\"sentence\"]\n",
    "#     if do_lower_case:\n",
    "#         transcription = transcription.lower()\n",
    "#     if do_remove_punctuation:\n",
    "#         transcription = normalizer(transcription).strip()\n",
    "    \n",
    "#     # encode target text to label ids\n",
    "#     batch[\"labels\"] = processor.tokenizer(transcription).input_ids\n",
    "#     return batch\n",
    "\n",
    "# max_label_length = 225 # model.config.max_length\n",
    "# min_input_length = 0.0\n",
    "# max_input_length = 30.0\n",
    "# def is_in_length_range(length, labels):\n",
    "#     return min_input_length < length < max_input_length and 0 < len(labels) < max_label_length\n",
    "\n",
    "\n",
    "# print('DATASET PREPARATION IN PROGRESS...')\n",
    "# raw_dataset = DatasetDict()\n",
    "# # raw_dataset[\"train\"] = load_custom_dataset('train')\n",
    "# raw_dataset[\"eval\"] = load_custom_dataset('eval')\n",
    "\n",
    "# raw_dataset = raw_dataset.cast_column(\"audio\", Audio(sampling_rate=args.sampling_rate))\n",
    "# raw_dataset = raw_dataset.map(prepare_dataset, num_proc=args.num_proc)\n",
    "\n",
    "# raw_dataset = raw_dataset.filter(\n",
    "#     is_in_length_range,\n",
    "#     input_columns=[\"input_length\", \"labels\"],\n",
    "#     num_proc=args.num_proc,\n",
    "# )\n",
    "\n",
    "# ###############################     DATA COLLATOR AND METRIC DEFINITION     ########################\n",
    "\n",
    "# @dataclass\n",
    "# class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "#     processor: Any\n",
    "\n",
    "#     def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "#         # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "#         # first treat the audio inputs by simply returning torch tensors\n",
    "#         input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "#         batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "#         # get the tokenized label sequences\n",
    "#         label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "#         # pad the labels to max length\n",
    "#         labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "#         # replace padding with -100 to ignore loss correctly\n",
    "#         labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "#         # if bos token is appended in previous tokenization step,\n",
    "#         # cut bos token here as it's append later anyways\n",
    "#         if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "#             labels = labels[:, 1:]\n",
    "\n",
    "#         batch[\"labels\"] = labels\n",
    "\n",
    "#         return batch\n",
    "\n",
    "# data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "# print('DATASET PREPARATION COMPLETED')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a69d6ec-56e4-4589-acca-4c7f2ee909e8",
   "metadata": {},
   "source": [
    "# separate different channels of wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d8d345e-c664-4366-9a8d-4d843138af8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # %pip install scipy\n",
    "\n",
    "# import numpy as np\n",
    "# import scipy.io.wavfile as wavfile\n",
    "\n",
    "# # Read the multi-channel WAV file\n",
    "# input_filename = 'data/midea_0612/wavs/d5f2afaa-53af-4dcb-ac24-3827a99c748e.wav'\n",
    "# sample_rate, data = wavfile.read(input_filename)\n",
    "\n",
    "# # Ensure the data is two-dimensional (i.e., multiple channels)\n",
    "# if len(data.shape) == 1:\n",
    "#     raise ValueError(\"The provided WAV file is not multi-channel.\")\n",
    "\n",
    "# # Get the number of channels\n",
    "# num_channels = data.shape[1]\n",
    "\n",
    "# # Loop through each channel and save it as a separate WAV file\n",
    "# for i in range(num_channels):\n",
    "#     channel_data = data[:, i]\n",
    "    \n",
    "#     output_filename = input_filename[:-4].replace('wavs', 'wavs_1channel')+f'_c{i}.wav'\n",
    "#     wavfile.write(output_filename, sample_rate, channel_data)\n",
    "#     print(f'Channel {i+1} saved as {output_filename}')\n",
    "\n",
    "\n",
    "# ## another way to separate different channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89241440-60c5-4189-ac9f-9a71f763b3f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1df03a-63bd-4edb-a1c1-90aa720cb86e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "def split_channels(audio_path, output_prefix):\n",
    "    # Load the multi-channel audio file\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "    # Get the number of channels\n",
    "    channels = audio.split_to_mono()\n",
    "    print(len(channels))\n",
    "\n",
    "    # Save each channel as a separate single-channel audio file\n",
    "    for i, channel in enumerate(channels):\n",
    "        output_path = f\"{output_prefix}_channel_{i+1}.wav\"\n",
    "        channel.export(output_path, format=\"wav\")\n",
    "        print(f\"Saved {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "audio_path = 'data/midea_0612/wavs/3ee4b9f4-7674-4978-9066-d89b46c9adb4.wav'\n",
    "output_prefix = \"data/midea_0612/wavs_1channel/3ee4b9f4-7674-4978-9066-d89b46c9adb4\"\n",
    "split_channels(audio_path, output_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89d76774-3349-4811-8c9e-90274df97d40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 ['f4857f5a-201d-4eea-ad3b-b8b048e72b68.wav', 'aec95ea8-e448-45a6-be21-63a6b6326b9f.wav', '8fcf8eb4-d4da-4146-a42c-d3ba711b4ed0.wav', 'd5f2afaa-53af-4dcb-ac24-3827a99c748e.wav', '9bdd9ccf-d94b-4f38-8a5d-35e4dd43aeee.wav']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:15<00:00,  1.18it/s]\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to split audio file into chunks of given duration\n",
    "def split_audio(file_path, chunk_length_ms=10000):\n",
    "    audio = AudioSegment.from_wav(file_path)\n",
    "    total_length_ms = len(audio)\n",
    "    \n",
    "    chunks = []\n",
    "    for start_ms in range(0, total_length_ms, chunk_length_ms):\n",
    "        end_ms = min(start_ms + chunk_length_ms, total_length_ms)\n",
    "        chunk = audio[start_ms:end_ms]\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Function to save audio chunks to files and separate channels\n",
    "def save_chunks_with_channels(chunks, output_dir, base_filename=\"chunk\", chunk_length=10):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Separate channels\n",
    "        left_channel = chunk.split_to_mono()[0]\n",
    "        right_channel = chunk.split_to_mono()[1]\n",
    "        \n",
    "        # Save left and right channels separately\n",
    "        left_filename = os.path.join(output_dir, f\"{base_filename}_{i*chunk_length}_customer.wav\")\n",
    "        right_filename = os.path.join(output_dir, f\"{base_filename}_{i*chunk_length}_agent.wav\")\n",
    "        \n",
    "        left_channel.export(left_filename, format=\"wav\")\n",
    "        right_channel.export(right_filename, format=\"wav\")\n",
    "        \n",
    "        # print(f\"Saved chunk {i} left channel: {left_filename}\")\n",
    "        # print(f\"Saved chunk {i} right channel: {right_filename}\")\n",
    "\n",
    "# Define the file path and output directory\n",
    "data_dir = 'data/midea_dialogue/long'\n",
    "filenames = [filename for filename in os.listdir('data/midea_dialogue/long') if filename.endswith('wav')]\n",
    "print(len(filenames), filenames[:5])\n",
    "\n",
    "chunk_length_ms = 30000 # ms\n",
    "\n",
    "for filename in tqdm(filenames, total=len(filenames)):\n",
    "    input_file_path = os.path.join(data_dir, filename)\n",
    "    output_directory = \"data/midea_dialogue/short_30s\"\n",
    "\n",
    "    # Split the audio file into 10-second chunks\n",
    "    chunks = split_audio(input_file_path, chunk_length_ms=chunk_length_ms)\n",
    "\n",
    "    basename = filename.split('.')[0]\n",
    "    # Save the chunks with separate channels\n",
    "    save_chunks_with_channels(chunks, output_directory, basename, chunk_length=(chunk_length_ms//1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df756e8b-2055-4894-9de7-2c358f2f0bce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_whisper",
   "language": "python",
   "name": "conda_whisper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
