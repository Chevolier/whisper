{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f40caf1-4209-48c6-961e-2e097aacae5a",
   "metadata": {},
   "source": [
    "# prepare text and audio_paths files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9345328-38bc-4d65-a911-46d0528dcd98",
   "metadata": {},
   "source": [
    "## preprocess audio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1797e649-f631-4908-bde2-957e7dbe32b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "seed = 17\n",
    "random.seed(seed)\n",
    "\n",
    "data_dir = \"data/midea_2173\"   # your midea data directory\n",
    "\n",
    "df_trans = pd.read_csv(os.path.join(data_dir, 'transcripts.csv'))\n",
    "\n",
    "print(df_trans.shape)\n",
    "df_trans.rename(columns={'content': 'sentence', 'record_file_id': 'file_id'}, inplace=True)\n",
    "df_trans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abeac1f-2e07-4944-af33-b6ab8fef9d00",
   "metadata": {},
   "source": [
    "## convert amr to wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21eea4c-bb0d-4bfc-bf10-308dca490041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465ca1a8-f847-4d34-95e6-2f6dfb4a1b31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i, row in tqdm(df_trans.iterrows(), total=df_trans.shape[0]):\n",
    "    filename = row['file_id'][:-3]\n",
    "    amr_audio = AudioSegment.from_file(data_dir+f'/amrs/{filename}amr', format=\"amr\")\n",
    "    amr_audio.export(data_dir+f'/wavs/{filename}wav', format=\"wav\")\n",
    "    df_trans.loc[i, 'file_id'] = f'{filename}wav'\n",
    "\n",
    "df_trans.to_csv(data_dir+'/transcripts.csv', index=False)\n",
    "df_trans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328714a0-9645-401d-8766-5c9f8386b168",
   "metadata": {},
   "source": [
    "## combine different datasets to form a custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eafa59-bd3c-46ca-9683-925efd039516",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# custom_dir = 'data/custom_data_v1' # 2k mdcc, 2k cmcc, 318 midea\n",
    "# custom_dir = 'data/custom_data_v2' # 65120 mdcc, 8429 cmcc, 300 midea\n",
    "# custom_dir = 'data/custom_data_v3' # 65120 mdcc, 8429 cmcc\n",
    "# custom_dir = 'data/custom_data_v4' # 5k mdcc, 5k cmcc\n",
    "# custom_dir = 'data/custom_data_v5' # 2k mdcc, 2k cmcc\n",
    "# custom_dir = 'data/custom_data_v6' # 5k mdcc, 5k cmcc, 318 midea\n",
    "custom_dir = 'data/custom_data_v7' # 5k mdcc, 5k cmcc, 1303 midea\n",
    "# custom_dir = 'data/custom_data_v8' # 1k mdcc, 1k cmcc, 1303 midea\n",
    "\n",
    "os.makedirs(custom_dir, exist_ok=True)\n",
    "\n",
    "text_path = os.path.join(custom_dir, 'text')\n",
    "audio_paths_path = os.path.join(custom_dir, 'audio_paths')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba2d104-f618-4344-a9d8-dc7f0084aae2",
   "metadata": {},
   "source": [
    "### MDCC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55640961-2c18-4f9c-a1ce-0b34ecc3fa90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import shutil\n",
    "\n",
    "mdcc_dir = 'data/MDCC'\n",
    "\n",
    "splits = ['train', 'valid', 'test']\n",
    "# counts = [2000, 200, 200]\n",
    "# counts = [65120, 5663, 12492]\n",
    "# counts = [65120, 5663, 12492]\n",
    "# counts = [5000, 1000, 1000]\n",
    "# counts = [2000, 200, 200]\n",
    "# counts = [5000, 200, 200]\n",
    "counts = [5000, 500, 500]\n",
    "# counts = [1000, 200, 200]\n",
    "\n",
    "for split, count in zip(splits, counts):\n",
    "    save_dir = os.path.join(custom_dir, split)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(mdcc_dir, f'cnt_asr_{split}_metadata.csv'))\n",
    "    print(split, df.shape)\n",
    "    print(df.head())\n",
    "    \n",
    "    df_sample = df.sample(n=min(count, df.shape[0]), random_state=seed)\n",
    "    with open(os.path.join(custom_dir, split, 'text'), 'a') as fo1, open(os.path.join(custom_dir, split, 'audio_paths'), 'a') as fo2:\n",
    "        for i, row in df_sample.iterrows():\n",
    "            unique_id = str(uuid.uuid4())\n",
    "            with open(os.path.join(mdcc_dir, row['text_path'][2:])) as fi:\n",
    "                trans = fi.read()\n",
    "                trans = trans.strip()\n",
    "\n",
    "            audio_path = os.path.realpath(mdcc_dir+'/'+row['audio_path'][2:])\n",
    "\n",
    "            # shutil.copy2(audio_src, audio_dest)\n",
    "            \n",
    "            # if i < df_sample.shape[0] - 1:\n",
    "            fo1.write(f\"{unique_id} {trans}\\n\")\n",
    "            fo2.write(f\"{unique_id} {audio_path}\\n\")\n",
    "            # else:\n",
    "            #     fo1.write(f\"{unique_id} {trans}\\n\")\n",
    "            #     fo2.write(f\"{unique_id} {audio_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35db05bb-87cf-468c-96f7-d1eee676d94c",
   "metadata": {},
   "source": [
    "## Common Voice 17.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf7268f-49b0-4062-ac19-dc058f226f35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import shutil\n",
    "\n",
    "common_dir = 'data/cv-corpus-17.0-2024-03-15/zh-HK'\n",
    "splits = ['train', 'valid', 'test']\n",
    "# counts = [2000, 200, 200]\n",
    "# counts = [8429, 5595, 5595]\n",
    "# counts = [8429, 5595, 5595]\n",
    "# counts = [5000, 1000, 1000]\n",
    "# counts = [2000, 200, 200]\n",
    "# counts = [5000, 200, 200]\n",
    "counts = [5000, 500, 500]\n",
    "# counts = [1000, 200, 200]\n",
    "\n",
    "for split, count in zip(splits, counts):\n",
    "    save_dir = os.path.join(custom_dir, split)\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(common_dir, f'{split}.tsv'), sep='\\t')\n",
    "    print(split, df.shape)\n",
    "    # print(df.head())\n",
    "    \n",
    "    df_sample = df.sample(n=min(count, df.shape[0]), random_state=seed)\n",
    "    with open(os.path.join(custom_dir, split, 'text'), 'a') as fo1, open(os.path.join(custom_dir, split, 'audio_paths'), 'a') as fo2:\n",
    "        for i, row in df_sample.iterrows():\n",
    "            unique_id = str(uuid.uuid4())\n",
    "            trans = row['sentence']\n",
    "            audio_path = os.path.realpath(common_dir+'/clips/'+row['path'])\n",
    "\n",
    "            # shutil.copy2(audio_src, audio_dest)\n",
    "\n",
    "            # if i < df_sample.shape[0] - 1:\n",
    "            fo1.write(f\"{unique_id} {trans}\\n\")\n",
    "            fo2.write(f\"{unique_id} {audio_path}\\n\")\n",
    "            # else:\n",
    "            #     fo1.write(f\"{unique_id} {trans}\\n\")\n",
    "            #     fo2.write(f\"{unique_id} {audio_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2512f129-1e80-4935-b1c6-0370b145a795",
   "metadata": {},
   "source": [
    "## Midea Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26808f1f-a3c4-473d-a4f8-1a41e30250a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import shutil\n",
    "\n",
    "midea_dir = 'data/midea_2173'\n",
    "splits = ['train', 'valid', 'test']\n",
    "ratios = [0.6, 0.2, 0.2]\n",
    "\n",
    "df = pd.read_csv(os.path.join(midea_dir, f'transcripts.csv'))\n",
    "df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "print(df.shape)\n",
    "print(df.head())\n",
    "\n",
    "prev_count = 0\n",
    "num = df.shape[0]\n",
    "for split, ratio in zip(splits, ratios):\n",
    "    save_dir = os.path.join(custom_dir, split)\n",
    "    \n",
    "    count = int(num*ratio)\n",
    "    print(split, count)\n",
    "\n",
    "    df_sample = df.loc[prev_count:prev_count+count]\n",
    "    prev_count += count\n",
    "    \n",
    "    with open(os.path.join(custom_dir, split, 'text'), 'a') as fo1, open(os.path.join(custom_dir, split, 'audio_paths'), 'a') as fo2:\n",
    "        for i, row in df_sample.iterrows():\n",
    "            unique_id = str(uuid.uuid4())\n",
    "            trans = row['sentence']\n",
    "            audio_path = os.path.realpath(midea_dir+'/wavs/'+row['file_id'])\n",
    "\n",
    "            # shutil.copy2(audio_src, audio_dest)\n",
    "\n",
    "            # if i < df_sample.shape[0] - 1:\n",
    "            fo1.write(f\"{unique_id} {trans}\\n\")\n",
    "            fo2.write(f\"{unique_id} {audio_path}\\n\")\n",
    "            # else:\n",
    "            #     fo1.write(f\"{unique_id} {trans}\")\n",
    "            #     fo2.write(f\"{unique_id} {audio_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06099b9-d91d-48a3-9e63-5e65e93d87ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # remove dirs\n",
    "\n",
    "# shutil.rmtree(custom_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510d87d6-4994-48fe-8bd7-a7e60db9c3ec",
   "metadata": {},
   "source": [
    "# process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ca7a5d-2f26-4e88-95c1-9cf37cfa7e22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 finetune/custom_data/data_prep.py \\\n",
    "--source_data_dir data/${custom_dir}/test \\\n",
    "--output_data_dir data/${custom_dir}/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4314d15-4099-4d1e-b013-467b132d0c29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 finetune/custom_data/data_prep.py \\\n",
    "--source_data_dir data/${custom_dir}/valid \\\n",
    "--output_data_dir data/${custom_dir}/valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8607b02d-ca92-4e7f-92a2-133de4668293",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 finetune/custom_data/data_prep.py \\\n",
    "--source_data_dir data/${custom_dir}/train \\\n",
    "--output_data_dir data/${custom_dir}/train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf62df4-4911-4fcf-a149-00bf8d545fa7",
   "metadata": {},
   "source": [
    "# Try to load the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a820b7-58e0-40b7-b33e-9ab3ccd38653",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import argparse\n",
    "# from datasets import DatasetDict, Audio, load_from_disk, concatenate_datasets\n",
    "\n",
    "# train_datasets = ['data/custom_data_v1/train']\n",
    "# eval_datasets = ['data/custom_data_v1/valid']\n",
    "\n",
    "# def load_custom_dataset(split):\n",
    "#     ds = []\n",
    "#     if split == 'train':\n",
    "#         for dset in train_datasets:\n",
    "#             ds.append(load_from_disk(dset))\n",
    "#     if split == 'eval':\n",
    "#         for dset in eval_datasets:\n",
    "#             ds.append(load_from_disk(dset))\n",
    "\n",
    "#     ds_to_return = concatenate_datasets(ds)\n",
    "#     ds_to_return = ds_to_return.shuffle(seed=22)\n",
    "#     return ds_to_return\n",
    "\n",
    "# ds_to_return = load_custom_dataset('eval')\n",
    "# ds_to_return\n",
    "\n",
    "# for ex in ds_to_return:\n",
    "#     print(ex)\n",
    "\n",
    "# def prepare_dataset(batch):\n",
    "#     # load and (possibly) resample audio data to 16kHz\n",
    "#     audio = batch[\"audio\"]\n",
    "\n",
    "#     # compute log-Mel input features from input audio array \n",
    "#     batch[\"input_features\"] = processor.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "#     # compute input length of audio sample in seconds\n",
    "#     batch[\"input_length\"] = len(audio[\"array\"]) / audio[\"sampling_rate\"]\n",
    "    \n",
    "#     # optional pre-processing steps\n",
    "#     transcription = batch[\"sentence\"]\n",
    "#     if do_lower_case:\n",
    "#         transcription = transcription.lower()\n",
    "#     if do_remove_punctuation:\n",
    "#         transcription = normalizer(transcription).strip()\n",
    "    \n",
    "#     # encode target text to label ids\n",
    "#     batch[\"labels\"] = processor.tokenizer(transcription).input_ids\n",
    "#     return batch\n",
    "\n",
    "# max_label_length = 225 # model.config.max_length\n",
    "# min_input_length = 0.0\n",
    "# max_input_length = 30.0\n",
    "# def is_in_length_range(length, labels):\n",
    "#     return min_input_length < length < max_input_length and 0 < len(labels) < max_label_length\n",
    "\n",
    "\n",
    "# print('DATASET PREPARATION IN PROGRESS...')\n",
    "# raw_dataset = DatasetDict()\n",
    "# # raw_dataset[\"train\"] = load_custom_dataset('train')\n",
    "# raw_dataset[\"eval\"] = load_custom_dataset('eval')\n",
    "\n",
    "# raw_dataset = raw_dataset.cast_column(\"audio\", Audio(sampling_rate=args.sampling_rate))\n",
    "# raw_dataset = raw_dataset.map(prepare_dataset, num_proc=args.num_proc)\n",
    "\n",
    "# raw_dataset = raw_dataset.filter(\n",
    "#     is_in_length_range,\n",
    "#     input_columns=[\"input_length\", \"labels\"],\n",
    "#     num_proc=args.num_proc,\n",
    "# )\n",
    "\n",
    "# ###############################     DATA COLLATOR AND METRIC DEFINITION     ########################\n",
    "\n",
    "# @dataclass\n",
    "# class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "#     processor: Any\n",
    "\n",
    "#     def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "#         # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "#         # first treat the audio inputs by simply returning torch tensors\n",
    "#         input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "#         batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "#         # get the tokenized label sequences\n",
    "#         label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "#         # pad the labels to max length\n",
    "#         labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "#         # replace padding with -100 to ignore loss correctly\n",
    "#         labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "#         # if bos token is appended in previous tokenization step,\n",
    "#         # cut bos token here as it's append later anyways\n",
    "#         if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "#             labels = labels[:, 1:]\n",
    "\n",
    "#         batch[\"labels\"] = labels\n",
    "\n",
    "#         return batch\n",
    "\n",
    "# data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "# print('DATASET PREPARATION COMPLETED')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a69d6ec-56e4-4589-acca-4c7f2ee909e8",
   "metadata": {},
   "source": [
    "# separate different channels of wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8d345e-c664-4366-9a8d-4d843138af8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # %pip install scipy\n",
    "\n",
    "# import numpy as np\n",
    "# import scipy.io.wavfile as wavfile\n",
    "\n",
    "# # Read the multi-channel WAV file\n",
    "# input_filename = 'data/midea_0612/wavs/d5f2afaa-53af-4dcb-ac24-3827a99c748e.wav'\n",
    "# sample_rate, data = wavfile.read(input_filename)\n",
    "\n",
    "# # Ensure the data is two-dimensional (i.e., multiple channels)\n",
    "# if len(data.shape) == 1:\n",
    "#     raise ValueError(\"The provided WAV file is not multi-channel.\")\n",
    "\n",
    "# # Get the number of channels\n",
    "# num_channels = data.shape[1]\n",
    "\n",
    "# # Loop through each channel and save it as a separate WAV file\n",
    "# for i in range(num_channels):\n",
    "#     channel_data = data[:, i]\n",
    "    \n",
    "#     output_filename = input_filename[:-4].replace('wavs', 'wavs_1channel')+f'_c{i}.wav'\n",
    "#     wavfile.write(output_filename, sample_rate, channel_data)\n",
    "#     print(f'Channel {i+1} saved as {output_filename}')\n",
    "\n",
    "\n",
    "# ## another way to separate different channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89241440-60c5-4189-ac9f-9a71f763b3f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1df03a-63bd-4edb-a1c1-90aa720cb86e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "def split_channels(audio_path, output_prefix):\n",
    "    # Load the multi-channel audio file\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "    # Get the number of channels\n",
    "    channels = audio.split_to_mono()\n",
    "    print(len(channels))\n",
    "\n",
    "    # Save each channel as a separate single-channel audio file\n",
    "    for i, channel in enumerate(channels):\n",
    "        output_path = f\"{output_prefix}_channel_{i+1}.wav\"\n",
    "        channel.export(output_path, format=\"wav\")\n",
    "        print(f\"Saved {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "audio_path = 'data/midea_0612/wavs/3ee4b9f4-7674-4978-9066-d89b46c9adb4.wav'\n",
    "output_prefix = \"data/midea_0612/wavs_1channel/3ee4b9f4-7674-4978-9066-d89b46c9adb4\"\n",
    "split_channels(audio_path, output_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89d76774-3349-4811-8c9e-90274df97d40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ['14468275-223c-41d7-af78-1e5abea9a8c7_313_agent.wav']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m basename \u001b[38;5;241m=\u001b[39m filename\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Save the chunks with separate channels\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[43msave_chunks_with_channels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk_length_ms\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 26\u001b[0m, in \u001b[0;36msave_chunks_with_channels\u001b[0;34m(chunks, output_dir, base_filename, chunk_length)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunks):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Separate channels\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     left_channel \u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39msplit_to_mono()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 26\u001b[0m     right_channel \u001b[38;5;241m=\u001b[39m \u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_to_mono\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Save left and right channels separately\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     left_filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m*\u001b[39mchunk_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_customer.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to split audio file into chunks of given duration\n",
    "def split_audio(file_path, chunk_length_ms=10000):\n",
    "    audio = AudioSegment.from_wav(file_path)\n",
    "    total_length_ms = len(audio)\n",
    "    \n",
    "    chunks = []\n",
    "    for start_ms in range(0, total_length_ms, chunk_length_ms):\n",
    "        end_ms = min(start_ms + chunk_length_ms, total_length_ms)\n",
    "        chunk = audio[start_ms:end_ms]\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Function to save audio chunks to files and separate channels\n",
    "def save_chunks_with_channels(chunks, output_dir, base_filename=\"chunk\", chunk_length=10):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Separate channels\n",
    "        left_channel = chunk.split_to_mono()[0]\n",
    "        right_channel = chunk.split_to_mono()[1]\n",
    "        \n",
    "        # Save left and right channels separately\n",
    "        left_filename = os.path.join(output_dir, f\"{base_filename}_{i*chunk_length}_customer.wav\")\n",
    "        right_filename = os.path.join(output_dir, f\"{base_filename}_{i*chunk_length}_agent.wav\")\n",
    "        \n",
    "        left_channel.export(left_filename, format=\"wav\")\n",
    "        right_channel.export(right_filename, format=\"wav\")\n",
    "        \n",
    "        # print(f\"Saved chunk {i} left channel: {left_filename}\")\n",
    "        # print(f\"Saved chunk {i} right channel: {right_filename}\")\n",
    "\n",
    "# Define the file path and output directory\n",
    "# data_dir = 'data/midea_dialogue/long'\n",
    "# data_dir = 'data/test_files_20240816/ai辅助填单数据/录音文件'\n",
    "data_dir = 'data/test_0823'\n",
    "\n",
    "filenames = [filename for filename in os.listdir(data_dir) if filename.endswith('wav')]\n",
    "print(len(filenames), filenames[:5])\n",
    "\n",
    "chunk_length_ms = 10000 # ms\n",
    "\n",
    "for filename in tqdm(filenames, total=len(filenames)):\n",
    "    input_file_path = os.path.join(data_dir, filename)\n",
    "    output_directory = \"data/test_0823/short_10s\"\n",
    "\n",
    "    # Split the audio file into 10-second chunks\n",
    "    chunks = split_audio(input_file_path, chunk_length_ms=chunk_length_ms)\n",
    "\n",
    "    basename = filename.split('.')[0]\n",
    "    # Save the chunks with separate channels\n",
    "    save_chunks_with_channels(chunks, output_directory, basename, chunk_length=(chunk_length_ms//1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ea7e170-49e8-4581-93a1-01e1f3be3023",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: data/test_0823/14468275-223c-41d7-af78-1e5abea9a8c7_313_agent.wav to s3://sagemaker-us-west-2-452145973879/datasets/midea_data/test_0823/short_10s/14468275-223c-41d7-af78-1e5abea9a8c7_313_agent.wav\n"
     ]
    }
   ],
   "source": [
    "!aws s3 sync data/test_0823 s3://sagemaker-us-west-2-452145973879/datasets/midea_data/test_0823/short_10s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e893f4-0b36-44e5-a84e-2f9e2316aa86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
