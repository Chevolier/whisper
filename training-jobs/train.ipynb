{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a473fe8-ef6b-49a4-b7d2-d96600d9163d",
   "metadata": {},
   "source": [
    "## 1. Upload data to S3\n",
    "Here I use pokeman dataset as an example, which is composed of 833 image-text pairs. To scale up, you can just process your data into the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60a3ee2c-be39-46cf-ae76-1ecfc52e0e24",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f972fb3-9c30-4c5c-adea-36ed78e38d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import datetime\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d4023f7-0ad4-460d-96f1-db5710f05d87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44e6697e-5363-4614-9371-284ec1b65fd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-452145973879/datasets/midea_data\n"
     ]
    }
   ],
   "source": [
    "prefix_data = 'datasets/midea_data'\n",
    "\n",
    "local_data_path = \"../data/midea_data\"\n",
    "input_data = sagemaker_session.upload_data(path=local_data_path, key_prefix=prefix_data)\n",
    "print(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc31ccb1-e849-43e0-85eb-9dd8b8984fa5",
   "metadata": {},
   "source": [
    "## 2. Upload pretrained models to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35dcff89-e473-497b-bc7a-4e333fbcc706",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-452145973879/models/whisper-large-v3\n"
     ]
    }
   ],
   "source": [
    "prefix_model = 'models/whisper-large-v3'\n",
    "\n",
    "local_model_path = \"/home/ec2-user/SageMaker/efs/Models/whisper-large-v3\"\n",
    "input_model = sagemaker_session.upload_data(path=local_model_path, key_prefix=prefix_model)\n",
    "print(input_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417bb61d-6527-4707-9cbb-185c89b51008",
   "metadata": {},
   "source": [
    "## 3. Start a training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8a06ebc-ab38-48b6-9e9e-9c3f0145440e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: whisper-launch-2024-06-06-15-30-50-374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-06 15:30:52 Starting - Starting the training job...\n",
      "2024-06-06 15:30:53 Pending - Training job waiting for capacity....................................................................................................................................................................................\n",
      "2024-06-06 16:01:07 Pending - Preparing the instances for training........................\n",
      "2024-06-06 16:05:18 Downloading - Downloading input data...\n",
      "2024-06-06 16:05:48 Downloading - Downloading the training image.........\n",
      "2024-06-06 16:07:34 Training - Training image download completed. Training in progress..........\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-06-06 16:08:51,363 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-06-06 16:08:51,469 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-06-06 16:08:51,478 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-06-06 16:08:51,480 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-06-06 16:08:53,123 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (0.59.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (2.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (4.66.4)\u001b[0m\n",
      "\u001b[34mCollecting more-itertools (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading more_itertools-10.2.0-py3-none-any.whl.metadata (34 kB)\u001b[0m\n",
      "\u001b[34mCollecting tiktoken (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: triton<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (2.2.2)\u001b[0m\n",
      "\u001b[34mCollecting jiwer (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading jiwer-3.0.4-py3-none-any.whl.metadata (2.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting evaluate (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting tensorboardX (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba->-r requirements.txt (line 1)) (0.42.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (3.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (4.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (3.1.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (2024.5.0)\u001b[0m\n",
      "\u001b[34mCollecting regex>=2022.1.18 (from tiktoken->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 6.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken->-r requirements.txt (line 6)) (2.32.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 8)) (2.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 8)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 8)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click<9.0.0,>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from jiwer->-r requirements.txt (line 9)) (8.1.7)\u001b[0m\n",
      "\u001b[34mCollecting rapidfuzz<4,>=3 (from jiwer->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading rapidfuzz-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting datasets>=2.0.0 (from evaluate->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.19.2-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 10)) (0.3.8)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from evaluate->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 10)) (0.70.16)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub>=0.7.0 (from evaluate->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 10)) (23.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.20 in /opt/conda/lib/python3.10/site-packages (from tensorboardX->-r requirements.txt (line 11)) (3.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate->-r requirements.txt (line 10)) (16.1.0)\u001b[0m\n",
      "\u001b[34mCollecting pyarrow-hotfix (from datasets>=2.0.0->evaluate->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec (from torch->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets>=2.0.0->evaluate->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate->-r requirements.txt (line 10)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 8)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 6)) (3.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 6)) (3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 6)) (1.26.18)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 6)) (2024.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->-r requirements.txt (line 3)) (2.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->-r requirements.txt (line 3)) (1.3.0)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 10)) (23.2.0)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0 (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mDownloading more_itertools-10.2.0-py3-none-any.whl (57 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.0/57.0 kB 1.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 30.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading jiwer-3.0.4-py3-none-any.whl (21 kB)\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.1/84.1 kB 17.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.7/101.7 kB 16.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.8/119.8 MB 25.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.19.2-py3-none-any.whl (542 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 542.1/542.1 kB 52.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 172.0/172.0 kB 24.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 401.7/401.7 kB 54.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading rapidfuzz-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 55.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 775.1/775.1 kB 69.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 26.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 79.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 239.5/239.5 kB 31.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.3/124.3 kB 2.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.6/301.6 kB 7.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: xxhash, tensorboardX, regex, rapidfuzz, pyarrow-hotfix, multidict, more-itertools, fsspec, frozenlist, async-timeout, yarl, tiktoken, jiwer, huggingface-hub, aiosignal, bitsandbytes, aiohttp, datasets, evaluate\u001b[0m\n",
      "\u001b[34mAttempting uninstall: fsspec\u001b[0m\n",
      "\u001b[34mFound existing installation: fsspec 2024.5.0\u001b[0m\n",
      "\u001b[34mUninstalling fsspec-2024.5.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled fsspec-2024.5.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 bitsandbytes-0.43.1 datasets-2.19.2 evaluate-0.4.2 frozenlist-1.4.1 fsspec-2024.3.1 huggingface-hub-0.23.3 jiwer-3.0.4 more-itertools-10.2.0 multidict-6.0.5 pyarrow-hotfix-0.6 rapidfuzz-3.9.3 regex-2024.5.15 tensorboardX-2.6.2.2 tiktoken-0.7.0 xxhash-3.4.1 yarl-1.9.4\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2024-06-06 16:09:02,686 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-06-06 16:09:02,687 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-06-06 16:09:02,810 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-06-06 16:09:02,914 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-06-06 16:09:03,017 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-06-06 16:09:03,027 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"whisper-launch-2024-06-06-15-30-50-374\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-452145973879/whisper-launch-2024-06-06-15-30-50-374/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"entry\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"entry.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=entry.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=entry\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-452145973879/whisper-launch-2024-06-06-15-30-50-374/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"whisper-launch-2024-06-06-15-30-50-374\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-452145973879/whisper-launch-2024-06-06-15-30-50-374/source/sourcedir.tar.gz\",\"module_name\":\"entry\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"entry.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 entry.py\u001b[0m\n",
      "\u001b[34m2024-06-06 16:09:03,028 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2024-06-06 16:09:03,028 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34msh: 1: wandb: not found\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (24.0)\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mCollecting git+https://github.com/huggingface/transformers.git\u001b[0m\n",
      "\u001b[34mCloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-iueuts_f\u001b[0m\n",
      "\u001b[34mRunning command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-iueuts_f\u001b[0m\n",
      "\u001b[34mResolved https://github.com/huggingface/transformers.git to commit b6c9f47fd6f911450024c52e382e544e5d04387a\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.22.0)\u001b[0m\n",
      "\u001b[34mCollecting accelerate\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.30.1-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets[audio] in /opt/conda/lib/python3.10/site-packages (2.19.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.42.0.dev0) (3.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.42.0.dev0) (0.23.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.42.0.dev0) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.42.0.dev0) (23.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.42.0.dev0) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.42.0.dev0) (2024.5.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.42.0.dev0) (2.32.2)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.20,>=0.19 (from transformers==4.42.0.dev0)\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.4.1 (from transformers==4.42.0.dev0)\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.42.0.dev0) (4.66.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets[audio]) (16.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets[audio]) (0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets[audio]) (0.3.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets[audio]) (2.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets[audio]) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets[audio]) (0.70.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets[audio]) (2024.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets[audio]) (3.9.5)\u001b[0m\n",
      "\u001b[34mCollecting soundfile>=0.12.1 (from datasets[audio])\u001b[0m\n",
      "\u001b[34mDownloading soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl.metadata (14 kB)\u001b[0m\n",
      "\u001b[34mCollecting librosa (from datasets[audio])\u001b[0m\n",
      "\u001b[34mDownloading librosa-0.10.2.post1-py3-none-any.whl.metadata (8.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets[audio]) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets[audio]) (23.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets[audio]) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets[audio]) (6.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets[audio]) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets[audio]) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.42.0.dev0) (4.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.42.0.dev0) (3.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.42.0.dev0) (3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.42.0.dev0) (1.26.18)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.42.0.dev0) (2024.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile>=0.12.1->datasets[audio]) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\u001b[0m\n",
      "\u001b[34mCollecting audioread>=2.1.9 (from librosa->datasets[audio])\u001b[0m\n",
      "\u001b[34mDownloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from librosa->datasets[audio]) (1.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from librosa->datasets[audio]) (1.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.10/site-packages (from librosa->datasets[audio]) (1.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from librosa->datasets[audio]) (5.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numba>=0.51.0 in /opt/conda/lib/python3.10/site-packages (from librosa->datasets[audio]) (0.59.1)\u001b[0m\n",
      "\u001b[34mCollecting pooch>=1.1 (from librosa->datasets[audio])\u001b[0m\n",
      "\u001b[34mDownloading pooch-1.8.1-py3-none-any.whl.metadata (9.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting soxr>=0.3.2 (from librosa->datasets[audio])\u001b[0m\n",
      "\u001b[34mDownloading soxr-0.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting lazy-loader>=0.1 (from librosa->datasets[audio])\u001b[0m\n",
      "\u001b[34mDownloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting msgpack>=1.0 (from librosa->datasets[audio])\u001b[0m\n",
      "\u001b[34mDownloading msgpack-1.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets[audio]) (2.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets[audio]) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets[audio]) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->datasets[audio]) (2.21)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.0->librosa->datasets[audio]) (0.42.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa->datasets[audio]) (4.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets[audio]) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->librosa->datasets[audio]) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.30.1-py3-none-any.whl (302 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 302.6/302.6 kB 9.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 45.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 80.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 110.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading librosa-0.10.2.post1-py3-none-any.whl (260 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 260.1/260.1 kB 38.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading audioread-3.0.1-py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mDownloading lazy_loader-0.4-py3-none-any.whl (12 kB)\u001b[0m\n",
      "\u001b[34mDownloading msgpack-1.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (385 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 385.1/385.1 kB 44.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading pooch-1.8.1-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.0/63.0 kB 12.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading soxr-0.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 80.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: transformers\u001b[0m\n",
      "\u001b[34mBuilding wheel for transformers (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for transformers (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for transformers: filename=transformers-4.42.0.dev0-py3-none-any.whl size=9142559 sha256=a7d21d588a5fa9bbb0073bf666d6c6dc0b766d0040535c82f5b49028c20e19aa\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-g4ogy1iy/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\u001b[0m\n",
      "\u001b[34mSuccessfully built transformers\u001b[0m\n",
      "\u001b[34mInstalling collected packages: soxr, safetensors, msgpack, lazy-loader, audioread, soundfile, pooch, tokenizers, librosa, accelerate, transformers\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.22.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.22.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.22.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.30.1 audioread-3.0.1 lazy-loader-0.4 librosa-0.10.2.post1 msgpack-1.0.8 pooch-1.8.1 safetensors-0.4.3 soundfile-0.12.1 soxr-0.3.7 tokenizers-0.19.1 transformers-4.42.0.dev0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mCollecting openai-whisper\u001b[0m\n",
      "\u001b[34mDownloading openai-whisper-20231117.tar.gz (798 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 798.6/798.6 kB 17.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: triton<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from openai-whisper) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from openai-whisper) (0.59.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from openai-whisper) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from openai-whisper) (2.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from openai-whisper) (4.66.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: more-itertools in /opt/conda/lib/python3.10/site-packages (from openai-whisper) (10.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tiktoken in /opt/conda/lib/python3.10/site-packages (from openai-whisper) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from triton<3,>=2.0.0->openai-whisper) (3.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba->openai-whisper) (0.42.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken->openai-whisper) (2024.5.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken->openai-whisper) (2.32.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->openai-whisper) (4.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->openai-whisper) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->openai-whisper) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->openai-whisper) (3.1.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->openai-whisper) (2024.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (1.26.18)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->openai-whisper) (2.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->openai-whisper) (1.3.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: openai-whisper\u001b[0m\n",
      "\u001b[34mBuilding wheel for openai-whisper (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for openai-whisper (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801358 sha256=209c67c90bdd7b20f42e848ea66605d76e66ae44f7cb9ccfe04ec8c1445cfa72\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/d0/85/e1/9361b4cbea7dd4b7f6702fa4c3afc94877952eeb2b62f45f56\u001b[0m\n",
      "\u001b[34mSuccessfully built openai-whisper\u001b[0m\n",
      "\u001b[34mInstalling collected packages: openai-whisper\u001b[0m\n",
      "\u001b[34mSuccessfully installed openai-whisper-20231117\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (0.59.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (2.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (4.66.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: more-itertools in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (10.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tiktoken in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: triton<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (2.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jiwer in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorboardX in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (2.6.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (0.43.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba->-r requirements.txt (line 1)) (0.42.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (3.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (4.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (3.1.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (2024.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken->-r requirements.txt (line 6)) (2024.5.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken->-r requirements.txt (line 6)) (2.32.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 8)) (2.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 8)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 8)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click<9.0.0,>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from jiwer->-r requirements.txt (line 9)) (8.1.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rapidfuzz<4,>=3 in /opt/conda/lib/python3.10/site-packages (from jiwer->-r requirements.txt (line 9)) (3.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 10)) (2.19.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 10)) (0.3.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 10)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 10)) (0.70.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 10)) (0.23.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 10)) (23.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.20 in /opt/conda/lib/python3.10/site-packages (from tensorboardX->-r requirements.txt (line 11)) (3.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate->-r requirements.txt (line 10)) (16.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate->-r requirements.txt (line 10)) (0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate->-r requirements.txt (line 10)) (3.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate->-r requirements.txt (line 10)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 8)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 6)) (3.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 6)) (3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 6)) (1.26.18)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 6)) (2024.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->-r requirements.txt (line 3)) (2.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->-r requirements.txt (line 3)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 10)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 10)) (23.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 10)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 10)) (6.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 10)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 10)) (4.0.3)\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mCollecting ffmpeg\u001b[0m\n",
      "\u001b[34mDownloading ffmpeg-1.4.tar.gz (5.1 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: ffmpeg\u001b[0m\n",
      "\u001b[34mBuilding wheel for ffmpeg (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for ffmpeg (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6080 sha256=1f2a3f4d07ccf0c04a02a128a42b9736a35d5b5e67b0e7fec44ec3a12cccf57f\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/8e/7a/69/cd6aeb83b126a7f04cbe7c9d929028dc52a6e7d525ff56003a\u001b[0m\n",
      "\u001b[34mSuccessfully built ffmpeg\u001b[0m\n",
      "\u001b[34mInstalling collected packages: ffmpeg\u001b[0m\n",
      "\u001b[34mSuccessfully installed ffmpeg-1.4\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[2024-06-06 16:09:41,133] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[34m[2024-06-06 16:09:41,133] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[2024-06-06 16:09:41,133] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m[2024-06-06 16:09:41,133] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[0m\n",
      "\u001b[34mARGUMENTS OF INTEREST:\u001b[0m\n",
      "\u001b[34m{'model_name': 'whisper-large-v3', 'language': 'Cantonese', 'sampling_rate': 16000, 'num_proc': 8, 'train_strategy': 'epoch', 'learning_rate': 0.003, 'warmup': 1000, 'train_batchsize': 1, 'eval_batchsize': 1, 'num_epochs': 2, 'num_steps': 100000, 'resume_from_ckpt': 'None', 'output_dir': '/opt/ml/checkpoints', 'train_datasets': ['data/midea_data'], 'eval_datasets': ['data/midea_data']}\u001b[0m\n",
      "\u001b[34m+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[0m\n",
      "\u001b[34mRunning on rank 2/8\u001b[0m\n",
      "\u001b[34m+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[0m\n",
      "\u001b[34mARGUMENTS OF INTEREST:\u001b[0m\n",
      "\u001b[34m{'model_name': 'whisper-large-v3', 'language': 'Cantonese', 'sampling_rate': 16000, 'num_proc': 8, 'train_strategy': 'epoch', 'learning_rate': 0.003, 'warmup': 1000, 'train_batchsize': 1, 'eval_batchsize': 1, 'num_epochs': 2, 'num_steps': 100000, 'resume_from_ckpt': 'None', 'output_dir': '/opt/ml/checkpoints', 'train_datasets': ['data/midea_data'], 'eval_datasets': ['data/midea_data']}\u001b[0m\n",
      "\u001b[34m+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[0m\n",
      "\u001b[34m+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[0m\n",
      "\u001b[34mARGUMENTS OF INTEREST:\u001b[0m\n",
      "\u001b[34m{'model_name': 'whisper-large-v3', 'language': 'Cantonese', 'sampling_rate': 16000, 'num_proc': 8, 'train_strategy': 'epoch', 'learning_rate': 0.003, 'warmup': 1000, 'train_batchsize': 1, 'eval_batchsize': 1, 'num_epochs': 2, 'num_steps': 100000, 'resume_from_ckpt': 'None', 'output_dir': '/opt/ml/checkpoints', 'train_datasets': ['data/midea_data'], 'eval_datasets': ['data/midea_data']}\u001b[0m\n",
      "\u001b[34m+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[0m\n",
      "\u001b[34mRunning on rank 1/8\u001b[0m\n",
      "\u001b[34mRunning on rank 7/8\u001b[0m\n",
      "\u001b[34m+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[0m\n",
      "\u001b[34mARGUMENTS OF INTEREST:\u001b[0m\n",
      "\u001b[34m{'model_name': 'whisper-large-v3', 'language': 'Cantonese', 'sampling_rate': 16000, 'num_proc': 8, 'train_strategy': 'epoch', 'learning_rate': 0.003, 'warmup': 1000, 'train_batchsize': 1, 'eval_batchsize': 1, 'num_epochs': 2, 'num_steps': 100000, 'resume_from_ckpt': 'None', 'output_dir': '/opt/ml/checkpoints', 'train_datasets': ['data/midea_data'], 'eval_datasets': ['data/midea_data']}\u001b[0m\n",
      "\u001b[34m+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[0m\n",
      "\u001b[34mRunning on rank 5/8\u001b[0m\n",
      "\u001b[34m+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[0m\n",
      "\u001b[34mARGUMENTS OF INTEREST:\u001b[0m\n",
      "\u001b[34m{'model_name': 'whisper-large-v3', 'language': 'Cantonese', 'sampling_rate': 16000, 'num_proc': 8, 'train_strategy': 'epoch', 'learning_rate': 0.003, 'warmup': 1000, 'train_batchsize': 1, 'eval_batchsize': 1, 'num_epochs': 2, 'num_steps': 100000, 'resume_from_ckpt': 'None', 'output_dir': '/opt/ml/checkpoints', 'train_datasets': ['data/midea_data'], 'eval_datasets': ['data/midea_data']}\u001b[0m\n",
      "\u001b[34m+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[0m\n",
      "\u001b[34m+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[0m\n",
      "\u001b[34mARGUMENTS OF INTEREST:\u001b[0m\n",
      "\u001b[34m{'model_name': 'whisper-large-v3', 'language': 'Cantonese', 'sampling_rate': 16000, 'num_proc': 8, 'train_strategy': 'epoch', 'learning_rate': 0.003, 'warmup': 1000, 'train_batchsize': 1, 'eval_batchsize': 1, 'num_epochs': 2, 'num_steps': 100000, 'resume_from_ckpt': 'None', 'output_dir': '/opt/ml/checkpoints', 'train_datasets': ['data/midea_data'], 'eval_datasets': ['data/midea_data']}\u001b[0m\n",
      "\u001b[34m+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[0m\n",
      "\u001b[34mRunning on rank 3/8\u001b[0m\n",
      "\u001b[34mRunning on rank 6/8\u001b[0m\n",
      "\u001b[34m+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[0m\n",
      "\u001b[34mARGUMENTS OF INTEREST:\u001b[0m\n",
      "\u001b[34m{'model_name': 'whisper-large-v3', 'language': 'Cantonese', 'sampling_rate': 16000, 'num_proc': 8, 'train_strategy': 'epoch', 'learning_rate': 0.003, 'warmup': 1000, 'train_batchsize': 1, 'eval_batchsize': 1, 'num_epochs': 2, 'num_steps': 100000, 'resume_from_ckpt': 'None', 'output_dir': '/opt/ml/checkpoints', 'train_datasets': ['data/midea_data'], 'eval_datasets': ['data/midea_data']}\u001b[0m\n",
      "\u001b[34m+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[0m\n",
      "\u001b[34mRunning on rank 4/8\u001b[0m\n",
      "\u001b[34m+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[0m\n",
      "\u001b[34mARGUMENTS OF INTEREST:\u001b[0m\n",
      "\u001b[34m{'model_name': 'whisper-large-v3', 'language': 'Cantonese', 'sampling_rate': 16000, 'num_proc': 8, 'train_strategy': 'epoch', 'learning_rate': 0.003, 'warmup': 1000, 'train_batchsize': 1, 'eval_batchsize': 1, 'num_epochs': 2, 'num_steps': 100000, 'resume_from_ckpt': 'None', 'output_dir': '/opt/ml/checkpoints', 'train_datasets': ['data/midea_data'], 'eval_datasets': ['data/midea_data']}\u001b[0m\n",
      "\u001b[34m+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[0m\n",
      "\u001b[34mRunning on rank 0/8\u001b[0m\n",
      "\u001b[34m*****************start cp data and pretrained models*****************************\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/midea_data/cache-9169d0adc5f7a64d.arrow data/midea_data/cache-9169d0adc5f7a64d.arrow\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/midea_data/.ipynb_checkpoints/state-checkpoint.json data/midea_data/.ipynb_checkpoints/state-checkpoint.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/midea_data/dataset_info.json data/midea_data/dataset_info.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/midea_data/cache-635e89158bf84b8d.arrow data/midea_data/cache-635e89158bf84b8d.arrow\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/midea_data/state.json data/midea_data/state.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/midea_data/cache-2179ed27affbc532_00002_of_00004.arrow data/midea_data/cache-2179ed27affbc532_00002_of_00004.arrow\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/midea_data/cache-eb747edc7aac02b5.arrow data/midea_data/cache-eb747edc7aac02b5.arrow\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/midea_data/cache-386bcc6bedae2c12.arrow data/midea_data/cache-386bcc6bedae2c12.arrow\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/midea_data/cache-af497a7d03c3cd4a.arrow data/midea_data/cache-af497a7d03c3cd4a.arrow\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/midea_data/.ipynb_checkpoints/dataset_info-checkpoint.json data/midea_data/.ipynb_checkpoints/dataset_info-checkpoint.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/midea_data/cache-d7fd3d823c462057.arrow data/midea_data/cache-d7fd3d823c462057.arrow\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/midea_data/cache-2179ed27affbc532_00003_of_00004.arrow data/midea_data/cache-2179ed27affbc532_00003_of_00004.arrow\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/midea_data/cache-97944ad7b8dfb755.arrow data/midea_data/cache-97944ad7b8dfb755.arrow\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/midea_data/cache-2179ed27affbc532_00001_of_00004.arrow data/midea_data/cache-2179ed27affbc532_00001_of_00004.arrow\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/midea_data/cache-c7ead2ed3a0db36a.arrow data/midea_data/cache-c7ead2ed3a0db36a.arrow\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/midea_data/cache-2179ed27affbc532_00000_of_00004.arrow data/midea_data/cache-2179ed27affbc532_00000_of_00004.arrow\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/midea_data/data-00000-of-00001.arrow data/midea_data/data-00000-of-00001.arrow\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/midea_data/cache-9ed63f1b5039e86c_00001_of_00004.arrow data/midea_data/cache-9ed63f1b5039e86c_00001_of_00004.arrow\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/midea_data/cache-9ed63f1b5039e86c_00003_of_00004.arrow data/midea_data/cache-9ed63f1b5039e86c_00003_of_00004.arrow\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/midea_data/cache-9ed63f1b5039e86c_00000_of_00004.arrow data/midea_data/cache-9ed63f1b5039e86c_00000_of_00004.arrow\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/midea_data/cache-9ed63f1b5039e86c_00002_of_00004.arrow data/midea_data/cache-9ed63f1b5039e86c_00002_of_00004.arrow\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/midea_data/cache-1537300377f31ea8.arrow data/midea_data/cache-1537300377f31ea8.arrow\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/midea_data/cache-59a4eae8d7676ccf.arrow data/midea_data/cache-59a4eae8d7676ccf.arrow\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/midea_data/cache-9ed63f1b5039e86c.arrow data/midea_data/cache-9ed63f1b5039e86c.arrow\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/midea_data/cache-af688477866cd9a8.arrow data/midea_data/cache-af688477866cd9a8.arrow\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/midea_data/cache-b127e27da0e87ea0.arrow data/midea_data/cache-b127e27da0e87ea0.arrow\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/datasets/midea_data/cache-da81647893983997.arrow data/midea_data/cache-da81647893983997.arrow\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/whisper-large-v3/generation_config.json whisper-large-v3/generation_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/whisper-large-v3/.gitattributes whisper-large-v3/.gitattributes\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/whisper-large-v3/preprocessor_config.json whisper-large-v3/preprocessor_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/whisper-large-v3/README.md whisper-large-v3/README.md\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/whisper-large-v3/normalizer.json whisper-large-v3/normalizer.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/whisper-large-v3/added_tokens.json whisper-large-v3/added_tokens.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/whisper-large-v3/special_tokens_map.json whisper-large-v3/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/whisper-large-v3/vocab.json whisper-large-v3/vocab.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/whisper-large-v3/merges.txt whisper-large-v3/merges.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/whisper-large-v3/pytorch_model.bin.index.fp32.json whisper-large-v3/pytorch_model.bin.index.fp32.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/whisper-large-v3/config.json whisper-large-v3/config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/whisper-large-v3/model.safetensors.index.fp32.json whisper-large-v3/model.safetensors.index.fp32.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/whisper-large-v3/tokenizer_config.json whisper-large-v3/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/whisper-large-v3/tokenizer.json whisper-large-v3/tokenizer.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/whisper-large-v3/model.fp32-00002-of-00002.safetensors whisper-large-v3/model.fp32-00002-of-00002.safetensors\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/whisper-large-v3/pytorch_model.fp32-00002-of-00002.bin whisper-large-v3/pytorch_model.fp32-00002-of-00002.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/whisper-large-v3/model.safetensors whisper-large-v3/model.safetensors\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/whisper-large-v3/pytorch_model.bin whisper-large-v3/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/whisper-large-v3/pytorch_model.fp32-00001-of-00002.bin whisper-large-v3/pytorch_model.fp32-00001-of-00002.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/whisper-large-v3/model.fp32-00001-of-00002.safetensors whisper-large-v3/model.fp32-00001-of-00002.safetensors\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-452145973879/models/whisper-large-v3/flax_model.msgpack whisper-large-v3/flax_model.msgpack\u001b[0m\n",
      "\u001b[34malgo-1:145:145 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:145:145 [0] NCCL INFO Bootstrap : Using eth0:10.0.250.206<0>\u001b[0m\n",
      "\u001b[34malgo-1:145:145 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:145:145 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34malgo-1:145:145 [0] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34mNCCL version 2.19.4+cuda12.1\u001b[0m\n",
      "\u001b[34malgo-1:147:147 [2] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34malgo-1:151:151 [6] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34malgo-1:146:146 [1] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34malgo-1:152:152 [7] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34malgo-1:147:147 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:150:150 [5] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34malgo-1:148:148 [3] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34malgo-1:151:151 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:146:146 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:149:149 [4] NCCL INFO cudaDriverVersion 12020\u001b[0m\n",
      "\u001b[34malgo-1:152:152 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:150:150 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:148:148 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:149:149 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:147:147 [2] NCCL INFO Bootstrap : Using eth0:10.0.250.206<0>\u001b[0m\n",
      "\u001b[34malgo-1:146:146 [1] NCCL INFO Bootstrap : Using eth0:10.0.250.206<0>\u001b[0m\n",
      "\u001b[34malgo-1:152:152 [7] NCCL INFO Bootstrap : Using eth0:10.0.250.206<0>\u001b[0m\n",
      "\u001b[34malgo-1:151:151 [6] NCCL INFO Bootstrap : Using eth0:10.0.250.206<0>\u001b[0m\n",
      "\u001b[34malgo-1:150:150 [5] NCCL INFO Bootstrap : Using eth0:10.0.250.206<0>\u001b[0m\n",
      "\u001b[34malgo-1:148:148 [3] NCCL INFO Bootstrap : Using eth0:10.0.250.206<0>\u001b[0m\n",
      "\u001b[34malgo-1:149:149 [4] NCCL INFO Bootstrap : Using eth0:10.0.250.206<0>\u001b[0m\n",
      "\u001b[34malgo-1:151:151 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:148:148 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:151:151 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34malgo-1:148:148 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34malgo-1:150:150 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:146:146 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:150:150 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34malgo-1:146:146 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34malgo-1:152:152 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:149:149 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:147:147 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:152:152 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34malgo-1:149:149 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34malgo-1:147:147 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO NET/OFI Internode latency set at 75.0 us\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO NET/OFI Selected Provider is efa (found 4 nics)\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO NET/OFI Internode latency set at 75.0 us\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO NET/OFI Internode latency set at 75.0 us\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO NET/OFI Internode latency set at 75.0 us\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO NET/OFI Internode latency set at 75.0 us\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO NET/OFI Internode latency set at 75.0 us\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO NET/OFI Internode latency set at 75.0 us\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO NET/OFI Selected Provider is efa (found 4 nics)\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO NET/OFI Selected Provider is efa (found 4 nics)\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO NET/OFI Selected Provider is efa (found 4 nics)\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO NET/OFI Selected Provider is efa (found 4 nics)\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO NET/OFI Selected Provider is efa (found 4 nics)\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO NET/OFI Selected Provider is efa (found 4 nics)\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO NET/OFI Using Libfabric version 1.20\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO NET/OFI Using CUDA driver version 12020\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO NET/OFI Internode latency set at 75.0 us\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO NET/OFI Selected Provider is efa (found 4 nics)\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Using non-device net plugin version 0\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO DMA-BUF is available on GPU device 0\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO DMA-BUF is available on GPU device 2\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO DMA-BUF is available on GPU device 1\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO DMA-BUF is available on GPU device 7\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO DMA-BUF is available on GPU device 4\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO DMA-BUF is available on GPU device 3\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO DMA-BUF is available on GPU device 5\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO DMA-BUF is available on GPU device 6\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO comm 0x55eba7a24590 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId a01d0 commId 0x2e557ff90381c3b6 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO comm 0x56372e280e30 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId a01c0 commId 0x2e557ff90381c3b6 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO comm 0x55af7c652ce0 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 901d0 commId 0x2e557ff90381c3b6 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO comm 0x55b15e307a00 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 901c0 commId 0x2e557ff90381c3b6 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO comm 0x55c9f0865a00 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 201d0 commId 0x2e557ff90381c3b6 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO comm 0x558760c2df10 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 201c0 commId 0x2e557ff90381c3b6 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO comm 0x559941255780 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 101d0 commId 0x2e557ff90381c3b6 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO comm 0x56537a2bc6f0 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 101c0 commId 0x2e557ff90381c3b6 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO NVLS multicast support is not available on dev 7\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO NVLS multicast support is not available on dev 2\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO NVLS multicast support is not available on dev 3\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO NVLS multicast support is not available on dev 1\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO NVLS multicast support is not available on dev 5\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO NVLS multicast support is not available on dev 6\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO NVLS multicast support is not available on dev 4\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO NVLS multicast support is not available on dev 0\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 00/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 01/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 02/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 03/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 04/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 05/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 06/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 07/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 08/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] -1/-1/-1->7->6 [7] -1/-1/-1->7->6 [8] -1/-1/-1->7->6 [9] -1/-1/-1->7->6 [10] -1/-1/-1->7->6 [11] -1/-1/-1->7->6 [12] -1/-1/-1->7->6 [13] -1/-1/-1->7->6 [14] -1/-1/-1->7->6 [15] -1/-1/-1->7->6 [16] -1/-1/-1->7->6 [17] -1/-1/-1->7->6 [18] -1/-1/-1->7->6 [19] -1/-1/-1->7->6 [20] -1/-1/-1->7->6 [21] -1/-1/-1->7->6 [22] -1/-1/-1->7->6 [23] -1/-1/-1->7->6\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 09/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 7/-1/-1->6->5 [9] 7/-1/-1->6->5 [10] 7/-1/-1->6->5 [11] 7/-1/-1->6->5 [12] 7/-1/-1->6->5 [13] 7/-1/-1->6->5 [14] 7/-1/-1->6->5 [15] 7/-1/-1->6->5 [16] 7/-1/-1->6->5 [17] 7/-1/-1->6->5 [18] 7/-1/-1->6->5 [19] 7/-1/-1->6->5 [20] 7/-1/-1->6->5 [21] 7/-1/-1->6->5 [22] 7/-1/-1->6->5 [23] 7/-1/-1->6->5\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 10/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 11/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 12/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->3 [7] 5/-1/-1->4->3 [8] 5/-1/-1->4->3 [9] 5/-1/-1->4->3 [10] 5/-1/-1->4->3 [11] 5/-1/-1->4->3 [12] 5/-1/-1->4->3 [13] 5/-1/-1->4->3 [14] 5/-1/-1->4->3 [15] 5/-1/-1->4->3 [16] 5/-1/-1->4->3 [17] 5/-1/-1->4->3 [18] 5/-1/-1->4->3 [19] 5/-1/-1->4->3 [20] 5/-1/-1->4->3 [21] 5/-1/-1->4->3 [22] 5/-1/-1->4->3 [23] 5/-1/-1->4->3\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 13/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] 6/-1/-1->5->4 [8] 6/-1/-1->5->4 [9] 6/-1/-1->5->4 [10] 6/-1/-1->5->4 [11] 6/-1/-1->5->4 [12] 6/-1/-1->5->4 [13] 6/-1/-1->5->4 [14] 6/-1/-1->5->4 [15] 6/-1/-1->5->4 [16] 6/-1/-1->5->4 [17] 6/-1/-1->5->4 [18] 6/-1/-1->5->4 [19] 6/-1/-1->5->4 [20] 6/-1/-1->5->4 [21] 6/-1/-1->5->4 [22] 6/-1/-1->5->4 [23] 6/-1/-1->5->4\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] 4/-1/-1->3->2 [7] 4/-1/-1->3->2 [8] 4/-1/-1->3->2 [9] 4/-1/-1->3->2 [10] 4/-1/-1->3->2 [11] 4/-1/-1->3->2 [12] 4/-1/-1->3->2 [13] 4/-1/-1->3->2 [14] 4/-1/-1->3->2 [15] 4/-1/-1->3->2 [16] 4/-1/-1->3->2 [17] 4/-1/-1->3->2 [18] 4/-1/-1->3->2 [19] 4/-1/-1->3->2 [20] 4/-1/-1->3->2 [21] 4/-1/-1->3->2 [22] 4/-1/-1->3->2 [23] 4/-1/-1->3->2\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 14/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 15/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 16/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 17/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 18/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 19/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 20/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 21/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 22/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 23/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 00/0 : 7[7] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 01/0 : 7[7] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 02/0 : 6[6] -> 7[7] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 02/0 : 7[7] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 02/0 : 3[3] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 02/0 : 4[4] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 02/0 : 5[5] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 03/0 : 6[6] -> 7[7] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 03/0 : 7[7] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 03/0 : 3[3] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 03/0 : 4[4] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 03/0 : 5[5] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 04/0 : 6[6] -> 7[7] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 04/0 : 7[7] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 04/0 : 3[3] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 04/0 : 4[4] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 04/0 : 5[5] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 05/0 : 6[6] -> 7[7] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 05/0 : 7[7] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 05/0 : 3[3] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 05/0 : 4[4] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 05/0 : 5[5] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 06/0 : 6[6] -> 7[7] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 06/0 : 7[7] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 06/0 : 3[3] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 06/0 : 4[4] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 06/0 : 5[5] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 07/0 : 6[6] -> 7[7] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 07/0 : 7[7] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 07/0 : 3[3] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 07/0 : 4[4] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 07/0 : 5[5] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 08/0 : 6[6] -> 7[7] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 08/0 : 7[7] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 08/0 : 3[3] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 08/0 : 4[4] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 08/0 : 5[5] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 09/0 : 6[6] -> 7[7] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 09/0 : 7[7] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 09/0 : 3[3] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 09/0 : 4[4] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 09/0 : 5[5] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 10/0 : 6[6] -> 7[7] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 10/0 : 7[7] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 10/0 : 3[3] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 10/0 : 4[4] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 10/0 : 5[5] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 11/0 : 6[6] -> 7[7] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 11/0 : 7[7] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 11/0 : 2[2] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 11/0 : 3[3] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 11/0 : 4[4] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 11/0 : 5[5] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 12/0 : 6[6] -> 7[7] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 12/0 : 7[7] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 12/0 : 3[3] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 12/0 : 4[4] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 12/0 : 5[5] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 13/0 : 6[6] -> 7[7] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 13/0 : 7[7] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 13/0 : 3[3] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 13/0 : 4[4] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 13/0 : 5[5] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 14/0 : 6[6] -> 7[7] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 14/0 : 7[7] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 14/0 : 3[3] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 14/0 : 4[4] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 14/0 : 5[5] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 15/0 : 6[6] -> 7[7] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 15/0 : 7[7] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 15/0 : 3[3] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 15/0 : 4[4] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 15/0 : 5[5] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 16/0 : 0[0] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 16/0 : 6[6] -> 7[7] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 16/0 : 7[7] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 16/0 : 2[2] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 16/0 : 3[3] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 16/0 : 4[4] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 16/0 : 5[5] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 17/0 : 0[0] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 17/0 : 6[6] -> 7[7] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 17/0 : 7[7] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 17/0 : 2[2] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 17/0 : 3[3] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 17/0 : 4[4] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 17/0 : 5[5] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 18/0 : 0[0] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 18/0 : 6[6] -> 7[7] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 18/0 : 7[7] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 18/0 : 2[2] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 18/0 : 3[3] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 18/0 : 4[4] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 18/0 : 5[5] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 19/0 : 0[0] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 19/0 : 6[6] -> 7[7] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 19/0 : 7[7] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 19/0 : 2[2] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 19/0 : 3[3] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 19/0 : 5[5] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 19/0 : 4[4] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 20/0 : 0[0] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 20/0 : 6[6] -> 7[7] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 20/0 : 7[7] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 20/0 : 2[2] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 20/0 : 3[3] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 20/0 : 5[5] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 20/0 : 4[4] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 21/0 : 0[0] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 21/0 : 6[6] -> 7[7] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 21/0 : 7[7] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 21/0 : 2[2] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 21/0 : 5[5] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 21/0 : 3[3] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 21/0 : 4[4] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 22/0 : 0[0] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 22/0 : 6[6] -> 7[7] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 22/0 : 7[7] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 22/0 : 2[2] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 22/0 : 5[5] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 22/0 : 3[3] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 22/0 : 4[4] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Channel 23/0 : 0[0] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 23/0 : 6[6] -> 7[7] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 23/0 : 7[7] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 23/0 : 2[2] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 23/0 : 5[5] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 23/0 : 3[3] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 23/0 : 4[4] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 08/0 : 1[1] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 09/0 : 1[1] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 10/0 : 1[1] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 11/0 : 1[1] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 16/0 : 1[1] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 17/0 : 1[1] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 18/0 : 1[1] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 19/0 : 1[1] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 20/0 : 1[1] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 21/0 : 1[1] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 22/0 : 1[1] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 23/0 : 1[1] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 02/0 : 4[4] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 03/0 : 4[4] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 04/0 : 4[4] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 05/0 : 4[4] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 06/0 : 4[4] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 07/0 : 4[4] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 08/0 : 4[4] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 02/0 : 6[6] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 09/0 : 4[4] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 03/0 : 6[6] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 10/0 : 4[4] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 02/0 : 7[7] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 04/0 : 6[6] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 11/0 : 4[4] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 03/0 : 7[7] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 05/0 : 6[6] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 12/0 : 4[4] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 04/0 : 7[7] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 06/0 : 6[6] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 13/0 : 4[4] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 05/0 : 7[7] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 07/0 : 6[6] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 14/0 : 4[4] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 06/0 : 7[7] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 08/0 : 6[6] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 15/0 : 4[4] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 07/0 : 7[7] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 09/0 : 6[6] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 02/0 : 5[5] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 16/0 : 4[4] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 08/0 : 7[7] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 03/0 : 5[5] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 10/0 : 6[6] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 17/0 : 4[4] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 09/0 : 7[7] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 04/0 : 5[5] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 11/0 : 6[6] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 18/0 : 4[4] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 10/0 : 7[7] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 12/0 : 6[6] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 05/0 : 5[5] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 19/0 : 4[4] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 11/0 : 7[7] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 13/0 : 6[6] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 06/0 : 5[5] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 20/0 : 4[4] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 12/0 : 7[7] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 14/0 : 6[6] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 07/0 : 5[5] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 21/0 : 4[4] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 13/0 : 7[7] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 08/0 : 5[5] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 15/0 : 6[6] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 22/0 : 4[4] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 14/0 : 7[7] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 09/0 : 5[5] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 16/0 : 6[6] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Channel 23/0 : 4[4] -> 3[3] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 15/0 : 7[7] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 17/0 : 6[6] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 10/0 : 5[5] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 18/0 : 6[6] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 11/0 : 5[5] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 19/0 : 6[6] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 12/0 : 5[5] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 20/0 : 6[6] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 13/0 : 5[5] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 21/0 : 6[6] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 16/0 : 7[7] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 14/0 : 5[5] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 22/0 : 6[6] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 17/0 : 7[7] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 15/0 : 5[5] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Channel 23/0 : 6[6] -> 5[5] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 18/0 : 7[7] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 16/0 : 5[5] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 08/0 : 2[2] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 19/0 : 7[7] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 17/0 : 5[5] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 20/0 : 7[7] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 18/0 : 5[5] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 10/0 : 2[2] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 21/0 : 7[7] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 19/0 : 5[5] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 22/0 : 7[7] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 20/0 : 5[5] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 12/0 : 2[2] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Channel 23/0 : 7[7] -> 6[6] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 21/0 : 5[5] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 13/0 : 2[2] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 22/0 : 5[5] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 14/0 : 2[2] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Channel 23/0 : 5[5] -> 4[4] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 15/0 : 2[2] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 16/0 : 2[2] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 08/0 : 3[3] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 17/0 : 2[2] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 18/0 : 2[2] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 19/0 : 2[2] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 20/0 : 2[2] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 21/0 : 2[2] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 11/0 : 3[3] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 22/0 : 2[2] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 12/0 : 3[3] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Channel 23/0 : 2[2] -> 1[1] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 13/0 : 3[3] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 14/0 : 3[3] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 15/0 : 3[3] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 16/0 : 3[3] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 17/0 : 3[3] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 18/0 : 3[3] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 19/0 : 3[3] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 20/0 : 3[3] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 21/0 : 3[3] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 22/0 : 3[3] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Channel 23/0 : 3[3] -> 2[2] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 16/0 : 1[1] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 17/0 : 1[1] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 18/0 : 1[1] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 19/0 : 1[1] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 20/0 : 1[1] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 21/0 : 1[1] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 22/0 : 1[1] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Channel 23/0 : 1[1] -> 0[0] via P2P/CUMEM/read\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:151:911 [6] NCCL INFO comm 0x56372e280e30 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId a01c0 commId 0x2e557ff90381c3b6 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:149:916 [4] NCCL INFO comm 0x55b15e307a00 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 901c0 commId 0x2e557ff90381c3b6 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:145:910 [0] NCCL INFO comm 0x56537a2bc6f0 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 101c0 commId 0x2e557ff90381c3b6 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:147:917 [2] NCCL INFO comm 0x558760c2df10 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 201c0 commId 0x2e557ff90381c3b6 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:150:915 [5] NCCL INFO comm 0x55af7c652ce0 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 901d0 commId 0x2e557ff90381c3b6 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:146:912 [1] NCCL INFO comm 0x559941255780 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 101d0 commId 0x2e557ff90381c3b6 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:148:913 [3] NCCL INFO comm 0x55c9f0865a00 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 201d0 commId 0x2e557ff90381c3b6 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:152:914 [7] NCCL INFO comm 0x55eba7a24590 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId a01d0 commId 0x2e557ff90381c3b6 - Init COMPLETE\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mDATASET PREPARATION IN PROGRESS...\u001b[0m\n",
      "\u001b[34mDATASET PREPARATION IN PROGRESS...\u001b[0m\n",
      "\u001b[34mDATASET PREPARATION IN PROGRESS...\u001b[0m\n",
      "\u001b[34mDATASET PREPARATION IN PROGRESS...\u001b[0m\n",
      "\u001b[34mDATASET PREPARATION IN PROGRESS...\u001b[0m\n",
      "\u001b[34mDATASET PREPARATION IN PROGRESS...\u001b[0m\n",
      "\u001b[34mDATASET PREPARATION IN PROGRESS...DATASET PREPARATION IN PROGRESS...\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   0%|          | 0/37 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   0%|          | 0/37 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   0%|          | 0/37 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   0%|          | 0/37 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   0%|          | 0/37 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   0%|          | 0/37 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   0%|          | 0/37 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   0%|          | 0/37 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   3%|▎         | 1/37 [00:07<04:20,  7.22s/ examples]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   3%|▎         | 1/37 [00:07<04:19,  7.22s/ examples]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   3%|▎         | 1/37 [00:07<04:19,  7.21s/ examples]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   3%|▎         | 1/37 [00:07<04:20,  7.24s/ examples]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   3%|▎         | 1/37 [00:07<04:20,  7.23s/ examples]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   3%|▎         | 1/37 [00:07<04:36,  7.69s/ examples]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   3%|▎         | 1/37 [00:07<04:20,  7.22s/ examples]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   3%|▎         | 1/37 [00:07<04:21,  7.27s/ examples]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  38%|███▊      | 14/37 [00:07<00:08,  2.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  51%|█████▏    | 19/37 [00:07<00:04,  3.63 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  32%|███▏      | 12/37 [00:07<00:11,  2.27 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  57%|█████▋    | 21/37 [00:07<00:03,  4.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  49%|████▊     | 18/37 [00:07<00:05,  3.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  38%|███▊      | 14/37 [00:07<00:08,  2.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  32%|███▏      | 12/37 [00:07<00:11,  2.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  38%|███▊      | 14/37 [00:07<00:08,  2.63 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  89%|████████▉ | 33/37 [00:07<00:00,  7.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  59%|█████▉    | 22/37 [00:07<00:03,  4.71 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  62%|██████▏   | 23/37 [00:07<00:02,  4.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  70%|███████   | 26/37 [00:07<00:01,  5.95 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  51%|█████▏    | 19/37 [00:07<00:04,  3.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  84%|████████▍ | 31/37 [00:08<00:00,  7.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  78%|███████▊  | 29/37 [00:07<00:01,  6.02 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  76%|███████▌  | 28/37 [00:07<00:01,  5.99 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  89%|████████▉ | 33/37 [00:07<00:00,  6.71 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  78%|███████▊  | 29/37 [00:07<00:01,  6.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  95%|█████████▍| 35/37 [00:07<00:00,  8.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  95%|█████████▍| 35/37 [00:07<00:00,  8.15 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  97%|█████████▋| 36/37 [00:07<00:00,  8.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  97%|█████████▋| 36/37 [00:07<00:00,  9.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  84%|████████▍ | 31/37 [00:07<00:00,  7.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8): 100%|██████████| 37/37 [00:08<00:00,  4.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8): 100%|██████████| 37/37 [00:08<00:00,  4.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8): 100%|██████████| 37/37 [00:08<00:00,  4.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8): 100%|██████████| 37/37 [00:08<00:00,  4.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8): 100%|██████████| 37/37 [00:08<00:00,  4.52 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8): 100%|██████████| 37/37 [00:08<00:00,  4.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8): 100%|██████████| 37/37 [00:08<00:00,  8.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8): 100%|██████████| 37/37 [00:08<00:00,  4.44 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8): 100%|██████████| 37/37 [00:08<00:00,  4.37 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):   0%|          | 0/37 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):   0%|          | 0/37 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):   0%|          | 0/37 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):   0%|          | 0/37 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):   0%|          | 0/37 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  14%|█▎        | 5/37 [00:00<00:00, 45.08 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):   0%|          | 0/37 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  14%|█▎        | 5/37 [00:00<00:00, 39.65 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  14%|█▎        | 5/37 [00:00<00:00, 41.39 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  14%|█▎        | 5/37 [00:00<00:00, 49.43 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):   0%|          | 0/37 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  14%|█▎        | 5/37 [00:00<00:00, 40.70 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8): 100%|██████████| 37/37 [00:00<00:00, 147.40 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):   0%|          | 0/37 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  14%|█▎        | 5/37 [00:00<00:00, 42.47 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8): 100%|██████████| 37/37 [00:00<00:00, 138.29 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8): 100%|██████████| 37/37 [00:00<00:00, 156.25 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  14%|█▎        | 5/37 [00:00<00:00, 36.83 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8): 100%|██████████| 37/37 [00:00<00:00, 127.68 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8): 100%|██████████| 37/37 [00:00<00:00, 139.18 examples/s]\u001b[0m\n",
      "\u001b[34mDATASET PREPARATION COMPLETED\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8): 100%|██████████| 37/37 [00:00<00:00, 142.76 examples/s]\u001b[0m\n",
      "\u001b[34mDATASET PREPARATION COMPLETED\u001b[0m\n",
      "\u001b[34mDATASET PREPARATION COMPLETED\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  14%|█▎        | 5/37 [00:00<00:01, 29.74 examples/s]\u001b[0m\n",
      "\u001b[34mDATASET PREPARATION COMPLETED\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8): 100%|██████████| 37/37 [00:00<00:00, 129.10 examples/s]\u001b[0m\n",
      "\u001b[34mDATASET PREPARATION COMPLETED\u001b[0m\n",
      "\u001b[34mDATASET PREPARATION COMPLETED\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8): 100%|██████████| 37/37 [00:00<00:00, 124.58 examples/s]\u001b[0m\n",
      "\u001b[34mDATASET PREPARATION COMPLETED\u001b[0m\n",
      "\u001b[34mDATASET PREPARATION COMPLETED\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/5.60k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 100%|██████████| 5.60k/5.60k [00:00<00:00, 32.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/5.60k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 100%|██████████| 5.60k/5.60k [00:00<00:00, 19.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/5.60k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 100%|██████████| 5.60k/5.60k [00:00<00:00, 24.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/5.60k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 100%|██████████| 5.60k/5.60k [00:00<00:00, 20.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/5.60k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 100%|██████████| 5.60k/5.60k [00:00<00:00, 15.2MB/s]\u001b[0m\n",
      "\u001b[34mTRAINING IN PROGRESS...\u001b[0m\n",
      "\u001b[34mTRAINING IN PROGRESS...\u001b[0m\n",
      "\u001b[34mTRAINING IN PROGRESS...\u001b[0m\n",
      "\u001b[34mTRAINING IN PROGRESS...\u001b[0m\n",
      "\u001b[34mTRAINING IN PROGRESS...\u001b[0m\n",
      "\u001b[34mTRAINING IN PROGRESS...\u001b[0m\n",
      "\u001b[34mTRAINING IN PROGRESS...\u001b[0m\n",
      "\u001b[34mTRAINING IN PROGRESS...\u001b[0m\n",
      "\u001b[34m0%|          | 0/10 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m10%|█         | 1/10 [00:01<00:14,  1.58s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 2/10 [00:02<00:07,  1.02it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 3/10 [00:02<00:05,  1.29it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 4/10 [00:03<00:04,  1.48it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 5/10 [00:03<00:03,  1.55it/s]\u001b[0m\n",
      "\u001b[34mDue to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\u001b[0m\n",
      "\u001b[34mDue to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\u001b[0m\n",
      "\u001b[34mDue to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\u001b[0m\n",
      "\u001b[34mDue to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\u001b[0m\n",
      "\u001b[34mDue to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\u001b[0m\n",
      "\u001b[34mDue to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\u001b[0m\n",
      "\u001b[34mDue to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\u001b[0m\n",
      "\u001b[34mDue to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\u001b[0m\n",
      "\u001b[34mDue to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\u001b[0m\n",
      "\u001b[34mDue to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\u001b[0m\n",
      "\u001b[34mDue to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\u001b[0m\n",
      "\u001b[34mDue to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\u001b[0m\n",
      "\u001b[34mDue to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\u001b[0m\n",
      "\u001b[34mDue to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\u001b[0m\n",
      "\u001b[34mDue to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\u001b[0m\n",
      "\u001b[34mDue to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\u001b[0m\n",
      "\u001b[34mThe attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\u001b[0m\n",
      "\u001b[34mThe attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\u001b[0m\n",
      "\u001b[34mThe attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\u001b[0m\n",
      "\u001b[34mThe attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\u001b[0m\n",
      "\u001b[34mThe attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\u001b[0m\n",
      "\u001b[34mThe attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\u001b[0m\n",
      "\u001b[34mThe attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\u001b[0m\n",
      "\u001b[34mThe attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\u001b[0m\n",
      "\u001b[34mThe attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\u001b[0m\n",
      "\u001b[34mThe attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\u001b[0m\n",
      "\u001b[34mThe attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\u001b[0m\n",
      "\u001b[34mThe attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\u001b[0m\n",
      "\u001b[34mThe attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\u001b[0m\n",
      "\u001b[34mThe attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\u001b[0m\n",
      "\u001b[34mThe attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\u001b[0m\n",
      "\u001b[34mThe attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\u001b[0m\n",
      "\u001b[34m0%|          | 0/5 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m40%|████      | 2/5 [00:01<00:02,  1.49it/s]#033[A\u001b[0m\n",
      "\u001b[34m60%|██████    | 3/5 [00:02<00:01,  1.15it/s]#033[A\u001b[0m\n",
      "\u001b[34m80%|████████  | 4/5 [00:03<00:00,  1.18it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 5/5 [00:04<00:00,  1.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 4.461463928222656, 'eval_cer': 43.737166324435314, 'eval_runtime': 24.1984, 'eval_samples_per_second': 1.529, 'eval_steps_per_second': 0.207, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m50%|█████     | 5/10 [00:27<00:03,  1.55it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 5/5 [00:22<00:00,  1.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\u001b[0m\n",
      "\u001b[34mNon-default generation parameters: {'max_length': 448, 'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}\u001b[0m\n",
      "\u001b[34mSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\u001b[0m\n",
      "\u001b[34mNon-default generation parameters: {'max_length': 448, 'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m60%|██████    | 6/10 [00:39<00:50, 12.58s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 7/10 [00:40<00:26,  8.68s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 8/10 [00:40<00:12,  6.11s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 9/10 [00:41<00:04,  4.40s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 10/10 [00:41<00:00,  3.19s/it]\u001b[0m\n",
      "\u001b[34mSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\u001b[0m\n",
      "\u001b[34mNon-default generation parameters: {'max_length': 448, 'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}\u001b[0m\n",
      "\u001b[34mSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\u001b[0m\n",
      "\u001b[34mNon-default generation parameters: {'max_length': 448, 'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}\u001b[0m\n",
      "\u001b[34m0%|          | 0/5 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m40%|████      | 2/5 [00:00<00:01,  2.67it/s]#033[A\u001b[0m\n",
      "\u001b[34m60%|██████    | 3/5 [00:01<00:01,  1.89it/s]#033[A\u001b[0m\n",
      "\u001b[34m80%|████████  | 4/5 [00:02<00:00,  1.61it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 5/5 [00:02<00:00,  1.59it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 1.2041394710540771, 'eval_cer': 22.381930184804926, 'eval_runtime': 22.4679, 'eval_samples_per_second': 1.647, 'eval_steps_per_second': 0.223, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 10/10 [01:14<00:00,  3.19s/it]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 5/5 [00:21<00:00,  1.59it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015                                             #033[A\u001b[0m\n",
      "\u001b[34mSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\u001b[0m\n",
      "\u001b[34mNon-default generation parameters: {'max_length': 448, 'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}\u001b[0m\n",
      "\u001b[34mSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\u001b[0m\n",
      "\u001b[34mNon-default generation parameters: {'max_length': 448, 'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}\u001b[0m\n",
      "\u001b[34mThere were missing keys in the checkpoint model loaded: ['proj_out.weight'].\u001b[0m\n",
      "\u001b[34mThere were missing keys in the checkpoint model loaded: ['proj_out.weight'].\u001b[0m\n",
      "\u001b[34mThere were missing keys in the checkpoint model loaded: ['proj_out.weight'].\u001b[0m\n",
      "\u001b[34mThere were missing keys in the checkpoint model loaded: ['proj_out.weight'].\u001b[0m\n",
      "\u001b[34mThere were missing keys in the checkpoint model loaded: ['proj_out.weight'].\u001b[0m\n",
      "\u001b[34mThere were missing keys in the checkpoint model loaded: ['proj_out.weight'].\u001b[0m\n",
      "\u001b[34mThere were missing keys in the checkpoint model loaded: ['proj_out.weight'].\u001b[0m\n",
      "\u001b[34mThere were missing keys in the checkpoint model loaded: ['proj_out.weight'].\u001b[0m\n",
      "\u001b[34mThere were missing keys in the checkpoint model loaded: ['proj_out.weight'].\u001b[0m\n",
      "\u001b[34mThere were missing keys in the checkpoint model loaded: ['proj_out.weight'].\u001b[0m\n",
      "\u001b[34mThere were missing keys in the checkpoint model loaded: ['proj_out.weight'].\u001b[0m\n",
      "\u001b[34mThere were missing keys in the checkpoint model loaded: ['proj_out.weight'].\u001b[0m\n",
      "\u001b[34mThere were missing keys in the checkpoint model loaded: ['proj_out.weight'].\u001b[0m\n",
      "\u001b[34mThere were missing keys in the checkpoint model loaded: ['proj_out.weight'].\u001b[0m\n",
      "\u001b[34mThere were missing keys in the checkpoint model loaded: ['proj_out.weight'].\u001b[0m\n",
      "\u001b[34mThere were missing keys in the checkpoint model loaded: ['proj_out.weight'].\u001b[0m\n",
      "\u001b[34mDONE TRAININGDONE TRAINING\u001b[0m\n",
      "\u001b[34mDONE TRAINING\u001b[0m\n",
      "\u001b[34mDONE TRAINING\u001b[0m\n",
      "\u001b[34mDONE TRAINING\u001b[0m\n",
      "\u001b[34mDONE TRAINING\u001b[0m\n",
      "\u001b[34m{'train_runtime': 157.1395, 'train_samples_per_second': 0.471, 'train_steps_per_second': 0.064, 'train_loss': 3.3730613708496096, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34mDONE TRAINING\u001b[0m\n",
      "\u001b[34m100%|██████████| 10/10 [02:37<00:00,  3.19s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 10/10 [02:37<00:00, 15.74s/it]\u001b[0m\n",
      "\u001b[34mDONE TRAINING\u001b[0m\n",
      "\u001b[34malgo-1:145:922 [0] NCCL INFO [Service thread] Connection closed by localRank 0\u001b[0m\n",
      "\u001b[34malgo-1:146:925 [1] NCCL INFO [Service thread] Connection closed by localRank 0\u001b[0m\n",
      "\u001b[34malgo-1:152:919 [7] NCCL INFO [Service thread] Connection closed by localRank 0\u001b[0m\n",
      "\u001b[34malgo-1:146:925 [1] NCCL INFO [Service thread] Connection closed by localRank 2\u001b[0m\n",
      "\u001b[34malgo-1:148:923 [3] NCCL INFO [Service thread] Connection closed by localRank 2\u001b[0m\n",
      "\u001b[34malgo-1:147:920 [2] NCCL INFO [Service thread] Connection closed by localRank 2\u001b[0m\n",
      "\u001b[34malgo-1:150:921 [5] NCCL INFO [Service thread] Connection closed by localRank 6\u001b[0m\n",
      "\u001b[34malgo-1:151:918 [6] NCCL INFO [Service thread] Connection closed by localRank 6\u001b[0m\n",
      "\u001b[34malgo-1:152:919 [7] NCCL INFO [Service thread] Connection closed by localRank 6\u001b[0m\n",
      "\u001b[34malgo-1:148:923 [3] NCCL INFO [Service thread] Connection closed by localRank 4\u001b[0m\n",
      "\u001b[34malgo-1:150:921 [5] NCCL INFO [Service thread] Connection closed by localRank 4\u001b[0m\n",
      "\u001b[34malgo-1:149:924 [4] NCCL INFO [Service thread] Connection closed by localRank 4\u001b[0m\n",
      "\u001b[34malgo-1:145:922 [0] NCCL INFO [Service thread] Connection closed by localRank 1\u001b[0m\n",
      "\u001b[34malgo-1:146:925 [1] NCCL INFO [Service thread] Connection closed by localRank 1\u001b[0m\n",
      "\u001b[34malgo-1:147:920 [2] NCCL INFO [Service thread] Connection closed by localRank 1\u001b[0m\n",
      "\u001b[34malgo-1:145:922 [0] NCCL INFO [Service thread] Connection closed by localRank 7\u001b[0m\n",
      "\u001b[34malgo-1:151:918 [6] NCCL INFO [Service thread] Connection closed by localRank 7\u001b[0m\n",
      "\u001b[34malgo-1:152:919 [7] NCCL INFO [Service thread] Connection closed by localRank 7\u001b[0m\n",
      "\u001b[34malgo-1:149:924 [4] NCCL INFO [Service thread] Connection closed by localRank 5\u001b[0m\n",
      "\u001b[34malgo-1:150:921 [5] NCCL INFO [Service thread] Connection closed by localRank 5\u001b[0m\n",
      "\u001b[34malgo-1:151:918 [6] NCCL INFO [Service thread] Connection closed by localRank 5\u001b[0m\n",
      "\u001b[34malgo-1:147:920 [2] NCCL INFO [Service thread] Connection closed by localRank 3\u001b[0m\n",
      "\u001b[34malgo-1:148:923 [3] NCCL INFO [Service thread] Connection closed by localRank 3\u001b[0m\n",
      "\u001b[34malgo-1:149:924 [4] NCCL INFO [Service thread] Connection closed by localRank 3\u001b[0m\n",
      "\u001b[34malgo-1:145:145 [0] NCCL INFO comm 0x56537a2bc6f0 rank 0 nranks 8 cudaDev 0 busId 101c0 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:151:151 [6] NCCL INFO comm 0x56372e280e30 rank 6 nranks 8 cudaDev 6 busId a01c0 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:147:147 [2] NCCL INFO comm 0x558760c2df10 rank 2 nranks 8 cudaDev 2 busId 201c0 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:149:149 [4] NCCL INFO comm 0x55b15e307a00 rank 4 nranks 8 cudaDev 4 busId 901c0 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:146:146 [1] NCCL INFO comm 0x559941255780 rank 1 nranks 8 cudaDev 1 busId 101d0 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:152:152 [7] NCCL INFO comm 0x55eba7a24590 rank 7 nranks 8 cudaDev 7 busId a01d0 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:150:150 [5] NCCL INFO comm 0x55af7c652ce0 rank 5 nranks 8 cudaDev 5 busId 901d0 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:148:148 [3] NCCL INFO comm 0x55c9f0865a00 rank 3 nranks 8 cudaDev 3 busId 201d0 - Abort COMPLETE\u001b[0m\n",
      "\u001b[34m2024-06-06 16:13:46,618 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-06-06 16:13:46,618 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-06-06 16:13:46,618 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-06-06 16:16:14 Uploading - Uploading generated training model\n",
      "2024-06-06 16:16:14 Completed - Training job completed\n",
      "Training seconds: 655\n",
      "Billable seconds: 655\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "region = sagemaker_session.boto_session.region_name\n",
    "\n",
    "image_uri = f'763104351884.dkr.ecr.{region}.amazonaws.com/pytorch-training:2.2.0-gpu-py310-cu121-ubuntu20.04-sagemaker'\n",
    "\n",
    "instance_count = 1\n",
    "# instance_type = 'ml.g5.48xlarge'\n",
    "instance_type = 'ml.p4d.24xlarge' # 8 x A100 40G\n",
    "\n",
    "checkpoint_local_path=\"/opt/ml/checkpoints\"\n",
    "\n",
    "environment = {\n",
    "    'NODE_NUMBER': str(instance_count),\n",
    "    'TRAIN_DATA_PATH': f's3://{bucket}/{prefix_data}/',\n",
    "    'VALID_DATA_PATH': f's3://{bucket}/{prefix_data}/',\n",
    "    'PRETRAINED_MODEL_S3_PATH': f\"{input_model}/\",\n",
    "    'OUTPUT_MODEL_S3_PATH': f's3://{bucket}/checkpoints/whisper_checkpoint', # destination\n",
    "}\n",
    "\n",
    "estimator = Estimator(role=role,\n",
    "                      entry_point='entry.py',\n",
    "                      source_dir='./sm_scripts',\n",
    "                      base_job_name='whisper-launch',\n",
    "                      instance_count=instance_count,\n",
    "                      instance_type=instance_type,\n",
    "                      volume_size=1024, # in GB\n",
    "                      image_uri=image_uri,\n",
    "                      environment=environment,\n",
    "                      max_run=3*24*3600, #任务最大存续时间，默认2day，需要提交ticket提升quota最大28天\n",
    "                      disable_profiler=True,\n",
    "                      debugger_hook_config=False,\n",
    "                      checkpoint_s3_bucket=f's3://{bucket}/checkpoints/whisper_checkpoint')\n",
    "\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4124e1c3-9f2f-4c6f-8f49-43311180565a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_dgmr_py310",
   "language": "python",
   "name": "conda_dgmr_py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
