{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ece8676-8984-4e84-b9aa-8f95a04b831a",
   "metadata": {
    "tags": []
   },
   "source": [
    "How to deploy whisper-large-v3 successfully by SageMaker notebook?\n",
    "\n",
    "\n",
    "When you try to deploy whisper-large-v3 by SageMaker notebook, you may get error like below:\n",
    "\n",
    "\n",
    "ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from primary with message \"{\n",
    "  \"code\": 400,\n",
    "  \"type\": \"InternalServerException\",\n",
    "  \"message\": \"Wrong index found for \\u003c|0.02|\\u003e: should be None but found 50366.\"\n",
    " \n",
    "\n",
    "In particular, it seems the AWS Deep Learning Containers only support up to transformers version 4.26.0, which is too low.\n",
    "\n",
    "You can find details here https://huggingface.co/openai/whisper-large-v3/discussions/58 Therefore, I’m sharing a workaround (custom way) to deploy the model or its derivatives.\n",
    "\n",
    "\n",
    "1. Create a notebook instance and a notebook file \n",
    "2. Execute following python code section by section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7644ef76-fe53-404b-ae41-97fffc110b92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3170c6-8a67-4e2d-8a80-b062a1f1326b",
   "metadata": {},
   "source": [
    "## Converting Checkpoint to faster-whisper checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "424165d6-5995-4b4d-b91a-2989f6b5856a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ckpt_dir = \"/home/ec2-user/SageMaker/efs/Projects/whisper/checkpoint/checkpoint-v7/checkpoint-60\"\n",
    "\n",
    "save_dir = \"./model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "037a9caf-a1e0-4877-bcd6-7a5e2e0d2e56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.55it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "!ct2-transformers-converter --model {ckpt_dir} --output_dir {save_dir} \\\n",
    "--copy_files tokenizer.json preprocessor_config.json --quantization float16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26340e26-f394-4de4-926a-db0cbfc601eb",
   "metadata": {},
   "source": [
    "# Prepare inference.py and requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8acf8d9-4f68-4e84-bab0-84714ab0da6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directory and file paths\n",
    "dir_path = './code'\n",
    "inference_file_path = os.path.join(dir_path, 'inference.py')\n",
    "requirements_file_path = os.path.join(dir_path, 'requirements.txt')\n",
    "\n",
    "# Create the directory structure\n",
    "os.makedirs(os.path.dirname(inference_file_path), exist_ok=True)\n",
    "\n",
    "# Inference.py content\n",
    "inference_content = '''\n",
    "from faster_whisper import WhisperModel\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "from decimal import Decimal\n",
    "def model_fn(model_dir):\n",
    "    # model_size = \"large-v3\"\n",
    "    ct_model_path=\".model\"\n",
    "    #model = WhisperModel(model_dir)\n",
    "    model = WhisperModel(model_dir, device=\"cuda\", compute_type=\"float16\")\n",
    "    #model = WhisperModel(model_dir, device=\"cpu\", compute_type=\"int8\")\n",
    "    return model\n",
    "\n",
    "def transform_fn(model, request_body, request_content_type, response_content_type=\"application/json\"):\n",
    "    print(f\"request_body:{request_body}\")\n",
    "    data = json.loads(request_body)\n",
    "    print(f\"type:{type(data)}\")\n",
    "    s3_client = boto3.client(\"s3\",region_name='us-west-2')  # us-west-2\n",
    "    s3_bucket = data['s3_bucket']\n",
    "    print(f\"s3_bucket:{type(s3_bucket)}\")\n",
    "    object_key = data['key']\n",
    "    print(f\"object_key:{type(object_key)}\")\n",
    "    audio_file_name = object_key[object_key.rfind('/')+1:]\n",
    "    contact_id = audio_file_name[0:audio_file_name.find(\"_\")]\n",
    "    fragment_id = audio_file_name[audio_file_name.find(\"_\")+1:audio_file_name.rfind(\"_\")]\n",
    "    s3_client.download_file(s3_bucket, object_key, f\"/tmp/{audio_file_name}\")\n",
    "    segments, info = model.transcribe(f\"/tmp/{audio_file_name}\",language=\"yue\",vad_filter=True, vad_parameters=dict(min_silence_duration_ms=100),)\n",
    "    scripts = []\n",
    "    speaker =\"\"\n",
    "    if audio_file_name.find(\"_agent.wav\") > 0:\n",
    "        speaker = \"agent\"\n",
    "    else:\n",
    "        speaker = \"customer\"\n",
    "    for segment in segments:\n",
    "        scripts.append({\"contact_id\":contact_id,\"time_stamp\":int(fragment_id)+segment.start,\"words\":str(segment.text),\"speaker\":speaker})\n",
    "    dynamodb_client = boto3.resource('dynamodb',region_name='us-west-2')\n",
    "    conversation_table = dynamodb_client.Table(\"speech2text\")\n",
    "    with conversation_table.batch_writer() as batch:\n",
    "        for item in scripts:\n",
    "            #print(item)\n",
    "            response = batch.put_item(Item={\n",
    "            \"ContactID\": item[\"contact_id\"],\n",
    "            \"SaidTimeStamp\": Decimal(str(item[\"time_stamp\"])),\n",
    "            \"Speaker\": item[\"speaker\"],\n",
    "            \"Words\": item[\"words\"]\n",
    "            })\n",
    "    #print(f\"result:{scripts}\")\n",
    "    os.remove(f\"/tmp/{audio_file_name}\")\n",
    "    #return scripts\n",
    "    return json.dumps(scripts), response_content_type\n",
    "\n",
    "'''\n",
    "\n",
    "# Write the inference.py file\n",
    "with open(inference_file_path, 'w') as file:\n",
    "    file.write(inference_content)\n",
    "\n",
    "# Requirements.txt content\n",
    "requirements_content = '''\n",
    "#transformers==4.36.2\n",
    "faster_whisper\n",
    "boto3\n",
    "ctranslate2==3.24.0\n",
    "#accelerate==0.26.1\n",
    "'''\n",
    "# Write the requirements.txt file\n",
    "with open(requirements_file_path, 'w') as file:\n",
    "    file.write(requirements_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87104966-0379-4437-944e-3d08dbb4dc79",
   "metadata": {},
   "source": [
    "# Prepare model and upload to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ef5b9a2-5df7-44c8-bb11-6bf115edfcf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/efs/Projects/whisper/sagemaker-deploy/faster-whisper/model.tar.gz'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.make_archive('./model', 'gztar', './model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82d80f30-f744-4a33-ba6c-3b1edff26be5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "bucket: sagemaker-us-west-2-452145973879, role:arn:aws:iam::452145973879:role/sagemaker_full_access, region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "# Get the SageMaker session and default S3 bucket\n",
    "# sagemaker_session = sagemaker.Session()\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = sagemaker_session._region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "print(f\"bucket: {bucket}, role:{role}, region: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef98d41a-0884-4fd6-8a18-4bdd6d7b6c58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model uploaded to s3://sagemaker-us-west-2-452145973879/models/whisper_ckpts/faster-v7-ckpt60/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "prefix = 'models/whisper_ckpts/faster-v7-ckpt60'\n",
    "\n",
    "# Upload the model to S3\n",
    "model_uri = sagemaker_session.upload_data(\n",
    "    'model.tar.gz',\n",
    "    bucket=bucket,\n",
    "    key_prefix=prefix\n",
    ")\n",
    "print(f\"Model uploaded to {model_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4ebf6ea-95ba-4873-b106-5007820aec39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm model.tar.gz\n",
    "!rm -rf model\n",
    "# model_uri = 's3://sagemaker-us-west-2-452145973879/models/whisper_ckpts/faster-whisper/model.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0815aa9-d97b-4456-ade8-5a786720d5d1",
   "metadata": {},
   "source": [
    "# Deploy faster-whisper to sagemaker endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "867abb93-be7e-449e-9a43-f2facf143372",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "import time\n",
    "\n",
    "id = int(time.time())\n",
    "endpoint_name = f'faster-whisper-hf-real-time-endpoint-{id}'\n",
    "\n",
    "huggingface_model = HuggingFaceModel(\n",
    "  model_data=model_uri,\n",
    "  entry_point=\"inference.py\",\n",
    "  source_dir='code',\n",
    "  role=role,\n",
    "  transformers_version='4.26',\n",
    "  pytorch_version='1.13',\n",
    "  py_version='py39',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f76c1dd-4e99-400c-881b-8e557a77b56b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "predictor = huggingface_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type='ml.g4dn.xlarge',\n",
    "  endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16cdd712-1684-412c-89c2-7592dd6a90ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '55a6b400-724a-4b4b-8043-22e14ab86f2e', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '55a6b400-724a-4b4b-8043-22e14ab86f2e', 'x-amzn-invoked-production-variant': 'AllTraffic', 'date': 'Thu, 27 Jun 2024 00:28:42 GMT', 'content-type': 'application/json', 'content-length': '2896', 'connection': 'keep-alive'}, 'RetryAttempts': 0}, 'ContentType': 'application/json', 'InvokedProductionVariant': 'AllTraffic', 'Body': <botocore.response.StreamingBody object at 0x7fe2d281ad40>}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import urllib.parse\n",
    "import boto3\n",
    "import os\n",
    "import logging\n",
    "\n",
    "#dynamodb_client = boto3.resource('dynamodb',region_name='us-west-2')\n",
    "#s3_client = boto3.client('s3',region_name='us-west-2')\n",
    "sagemaker_runtime= boto3.client('runtime.sagemaker',region_name=region)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(\"INFO\")\n",
    "#conversation_table = dynamodb_client.Table(\"speech2text\")\n",
    "s3_bucket = bucket\n",
    "\n",
    "# here replace the key with your test example s3 path\n",
    "key = urllib.parse.unquote_plus(\"datasets/wavs/d5f2afaa-53af-4dcb-ac24-3827a99c748e_1_customer.wav\", encoding='utf-8')\n",
    "try:\n",
    "    logger.info(\"s3_bucket:\"+s3_bucket)\n",
    "    logger.info(\"key:\"+key)\n",
    "    data = json.dumps({\"s3_bucket\":s3_bucket,\"key\":key})\n",
    "    response = sagemaker_runtime.invoke_endpoint(EndpointName=endpoint_name,ContentType='application/json',Body=data)\n",
    "        #logging(\"type:\"+type(response))\n",
    "        #logging(\"response:\"+str(response))\n",
    "        #results = json.loads(response['Body'].read().decode())\n",
    "        #response = s3_client.get_object(Bucket=s3_bucket, Key=key)\n",
    "    '''\n",
    "        with conversation_table.batch_writer() as batch:\n",
    "            for item in results:\n",
    "                print(item)\n",
    "                response = batch.put_item(Item={\n",
    "                \"ContactID\": item[\"contact_id\"],\n",
    "                \"SaidTimeStamp\": item[\"time_stamp\"],\n",
    "                \"Speaker\": item[\"speaker\"],\n",
    "                \"Words\": item[\"words\"]\n",
    "                })\n",
    "        #return response['ContentType']\n",
    "    '''\n",
    "    #return \"success\"\n",
    "    print(response)\n",
    "except:\n",
    "    logger.info(\"ignore known defects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b81d15-a4ac-446e-ad5a-2174f725c9c4",
   "metadata": {},
   "source": [
    "# Delete Items in DynamoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "431e6e2f-c2e2-4813-b23a-ae8c12a5b184",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All items in the table 'speech2text' have been deleted.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "region = 'us-west-2'\n",
    "\n",
    "table_name = \"speech2text\"\n",
    "dynamodb_client = boto3.resource('dynamodb',region_name='us-west-2')\n",
    "table = dynamodb_client.Table(table_name)\n",
    "\n",
    "# Scan the table to get all the items\n",
    "scan = table.scan()\n",
    "with table.batch_writer() as batch:\n",
    "    while 'Items' in scan and len(scan['Items']) > 0:\n",
    "        for item in scan['Items']:\n",
    "            # print(item)\n",
    "            # Delete each item\n",
    "            batch.delete_item(Key={'ContactID': item['ContactID'], 'SaidTimeStamp': item['SaidTimeStamp']})\n",
    "        # Scan for more items (DynamoDB can limit the number of items returned by scan, use 'ExclusiveStartKey' to continue scanning)\n",
    "        if 'LastEvaluatedKey' in scan:\n",
    "            scan = table.scan(ExclusiveStartKey=scan['LastEvaluatedKey'])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "print(f\"All items in the table '{table_name}' have been deleted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fa1227-1786-4ebc-b9da-1460e655884d",
   "metadata": {},
   "source": [
    "# Massive Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ca99d5d-461e-4a22-abb1-7663186d95e5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def list_s3_contents(bucket_name, prefix):\n",
    "    # Initialize a session using Amazon S3\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    # Initialize the variables\n",
    "    continuation_token = None\n",
    "    contents = []\n",
    "\n",
    "    while True:\n",
    "        if continuation_token:\n",
    "            response = s3.list_objects_v2(\n",
    "                Bucket=bucket_name, \n",
    "                Prefix=prefix,\n",
    "                ContinuationToken=continuation_token\n",
    "            )\n",
    "        else:\n",
    "            response = s3.list_objects_v2(\n",
    "                Bucket=bucket_name, \n",
    "                Prefix=prefix\n",
    "            )\n",
    "\n",
    "        if 'Contents' in response:\n",
    "            contents.extend(response['Contents'])\n",
    "\n",
    "        if response.get('IsTruncated'):  # if the response is truncated, there are more keys to retrieve\n",
    "            continuation_token = response.get('NextContinuationToken')\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return contents\n",
    "\n",
    "# Define the bucket name and prefix\n",
    "s3_uri = 's3://sagemaker-us-west-2-452145973879/datasets/midea_data/midea_dialogue/short_30s/'\n",
    "bucket_name = s3_uri.split('/')[2]\n",
    "prefix = '/'.join(s3_uri.split('/')[3:])\n",
    "\n",
    "# List the contents\n",
    "contents = list_s3_contents(bucket_name, prefix)\n",
    "# for obj in contents:\n",
    "#     print(obj['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f61fcf7-fd03-411d-8745-fdd88f2b0261",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 430/450 [17:38<00:48,  2.40s/it]"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import urllib.parse\n",
    "import boto3\n",
    "import os\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "#dynamodb_client = boto3.resource('dynamodb',region_name='us-west-2')\n",
    "#s3_client = boto3.client('s3',region_name='us-west-2')\n",
    "sagemaker_runtime= boto3.client('runtime.sagemaker',region_name=region)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(\"INFO\")\n",
    "#conversation_table = dynamodb_client.Table(\"speech2text\")\n",
    "s3_bucket = bucket\n",
    "\n",
    "for content in tqdm(contents, total=len(contents)):\n",
    "    key = urllib.parse.unquote_plus(content['Key'], encoding='utf-8')\n",
    "    try:\n",
    "        logger.info(\"s3_bucket:\"+s3_bucket)\n",
    "        logger.info(\"key:\"+key)\n",
    "        data = json.dumps({\"s3_bucket\":s3_bucket,\"key\":key})\n",
    "        response = sagemaker_runtime.invoke_endpoint(EndpointName=endpoint_name,ContentType='application/json',Body=data)\n",
    "            #logging(\"type:\"+type(response))\n",
    "            #logging(\"response:\"+str(response))\n",
    "            #results = json.loads(response['Body'].read().decode())\n",
    "            #response = s3_client.get_object(Bucket=s3_bucket, Key=key)\n",
    "        '''\n",
    "            with conversation_table.batch_writer() as batch:\n",
    "                for item in results:\n",
    "                    print(item)\n",
    "                    response = batch.put_item(Item={\n",
    "                    \"ContactID\": item[\"contact_id\"],\n",
    "                    \"SaidTimeStamp\": item[\"time_stamp\"],\n",
    "                    \"Speaker\": item[\"speaker\"],\n",
    "                    \"Words\": item[\"words\"]\n",
    "                    })\n",
    "            #return response['ContentType']\n",
    "        '''\n",
    "        #return \"success\"\n",
    "        # print(response)\n",
    "    except:\n",
    "        logger.info(\"ignore known defects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a21c23-ce6e-4ab6-91c9-95eb7d24b6f5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize a session using Amazon DynamoDB\n",
    "dynamodb = boto3.resource('dynamodb')\n",
    "\n",
    "# Specify the DynamoDB table\n",
    "table_name = 'speech2text'\n",
    "table = dynamodb.Table(table_name)\n",
    "\n",
    "# Scan the table\n",
    "def scan_table(table):\n",
    "    response = table.scan()\n",
    "    data = response['Items']\n",
    "\n",
    "    while 'LastEvaluatedKey' in response:\n",
    "        response = table.scan(ExclusiveStartKey=response['LastEvaluatedKey'])\n",
    "        data.extend(response['Items'])\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Get items from the table\n",
    "items = scan_table(table)\n",
    "\n",
    "# # Print the items\n",
    "# for item in items:\n",
    "#     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd2a9da-2a59-4a10-affe-b130d4767fa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from decimal import Decimal\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df = pd.DataFrame(items)\n",
    "# Sort the DataFrame by 'ContactID' and 'SaidTimeStamp'\n",
    "df = df.sort_values(by=['ContactID', 'SaidTimeStamp'])\n",
    "\n",
    "# Group by 'ContactID' and concatenate 'Words' with 'Speaker' prefix\n",
    "grouped = df.groupby('ContactID').apply(\n",
    "    lambda x: ' '.join(f\"{row['Speaker']}: {row['Words']}\\n\" for _, row in x.iterrows())\n",
    ").reset_index(name='content')\n",
    "\n",
    "# Rename the columns to match the image\n",
    "ckpt_name = 'v7_ckpt60'\n",
    "grouped.columns = ['contact_id', f'content_{ckpt_name}']\n",
    "\n",
    "grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b53709-3a88-4ebb-9fb7-d4d5ebed817b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_old = pd.read_csv(\"/home/ec2-user/SageMaker/efs/Projects/Qwen2/data/对话文本.csv\")\n",
    "df_old.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde7cda9-d8c8-4f23-846a-08b6bc0252e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_combine = pd.merge(df_old, grouped, on='contact_id')\n",
    "df_combine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a53620-6d69-4a02-80fd-f514ce336d79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_combine = df_combine[['param_id', 'contact_id', 'content', f'content_{ckpt_name}', 'extracted_content']]\n",
    "\n",
    "df_combine.to_csv(f\"../../outputs/transcripts_{ckpt_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da404450-4880-4ee7-b90f-1d3ae7185d6e",
   "metadata": {},
   "source": [
    "# Delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1f5724-5462-4a6c-acde-30b07b3d436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.delete_model()\n",
    "# predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813332a9-53db-4844-8c6e-036ec45a0e01",
   "metadata": {},
   "source": [
    "You may need to slice the audio files by seconds. Here are python to do the job\n",
    "\n",
    "https://github.com/Mohamedhany99/Audio-Splitter-per-seconds-python-/blob/main/Splitter.py\n",
    "https://www.geeksforgeeks.org/cut-a-mp3-file-in-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_whisper",
   "language": "python",
   "name": "conda_whisper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
